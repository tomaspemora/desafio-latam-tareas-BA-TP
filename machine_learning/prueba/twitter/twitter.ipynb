{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "# ------------------------------------------\n",
    "import nltk\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "## PROPIAS\n",
    "from utils import RemoveStopWords\n",
    "from utils import FeatureExtractionTwitts\n",
    "from utils import LemmantizerTransformer\n",
    "from utils import Vectorizer\n",
    "from utils import ColumnSelectedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_tweets.csv', index_col = 0)\n",
    "df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content.to_csv('raw_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_classes = {\n",
    "                    'happiness' : 'positiva',\n",
    "                    'surprise'  : 'positiva',\n",
    "                    'love'      : 'positiva',\n",
    "                    'fun'       : 'positiva',\n",
    "                    'relief'    : 'positiva',\n",
    "                    'enthusiasm': 'positiva',\n",
    "                    'worry'     : 'negativa',\n",
    "                    'hate'      : 'negativa',\n",
    "                    'sadness'   : 'negativa',\n",
    "                    'empty'     : 'negativa',\n",
    "                    'boredom'   : 'negativa',\n",
    "                    'anger'     : 'negativa',\n",
    "                    'neutral'   : 'neutral'\n",
    "                }\n",
    "neutral_class_name = 'neutral'\n",
    "target_var_name = 'sentiment'\n",
    "target_mapping = [('positiva',1),('negativa',0)]\n",
    "chars_to_replace = [\n",
    "                        ('ï¿½',''),\n",
    "                        ('&quot;',''),\n",
    "                        ('&lt;3','<3'), \n",
    "                        ('&lt;/3','</3'), \n",
    "                        ('&amp;','&'),\n",
    "                        ('&gt;','>'),\n",
    "                        ('&lt;','<')\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_remapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34723</th>\n",
       "      <td>Happy Mama's day to all mothers</td>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17493</th>\n",
       "      <td>@LysdelTellez I am lost. Please help me find a...</td>\n",
       "      <td>worry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20198</th>\n",
       "      <td>@BoomKatt yes yes I AM, networking whore to th...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>you@snapplynn Wish that would have been your t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>now i am doing the MicroEconomics project  iha...</td>\n",
       "      <td>worry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>I  do not want to work tomorrow!</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36006</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its s...</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22647</th>\n",
       "      <td>Photo: Got my prints a few days ago, ready for...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21478</th>\n",
       "      <td>@tove_liden Thanks for the follow Tove!</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39364</th>\n",
       "      <td>@esmeeworld thanks</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  sentiment  \\\n",
       "34723                    Happy Mama's day to all mothers       love   \n",
       "17493  @LysdelTellez I am lost. Please help me find a...      worry   \n",
       "20198  @BoomKatt yes yes I AM, networking whore to th...  happiness   \n",
       "6855   you@snapplynn Wish that would have been your t...    neutral   \n",
       "5924   now i am doing the MicroEconomics project  iha...      worry   \n",
       "...                                                  ...        ...   \n",
       "16469                   I  do not want to work tomorrow!    sadness   \n",
       "36006  @KandyBee we shuld do  a dance like that its s...        fun   \n",
       "22647  Photo: Got my prints a few days ago, ready for...  happiness   \n",
       "21478            @tove_liden Thanks for the follow Tove!        fun   \n",
       "39364                                 @esmeeworld thanks    neutral   \n",
       "\n",
       "      sentiment_remapped  \n",
       "34723                  1  \n",
       "17493                  0  \n",
       "20198                  1  \n",
       "6855                   1  \n",
       "5924                   0  \n",
       "...                  ...  \n",
       "16469                  0  \n",
       "36006                  1  \n",
       "22647                  1  \n",
       "21478                  1  \n",
       "39364                  0  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "df_preprocess = Pipeline(steps=[\n",
    "                ('regroup_classes', \n",
    "                    FunctionTransformer(\n",
    "                        utils.multi_class_remapping, \n",
    "                        kw_args={\n",
    "                            'group_classes': groups_classes,\n",
    "                            'var_name': target_var_name,\n",
    "                            'neutral_class': neutral_class_name,\n",
    "                            'random_state': 42\n",
    "                            })),\n",
    "                ('encoding', FunctionTransformer(\n",
    "                        utils.target_encoding,\n",
    "                        kw_args={\n",
    "                            'column_to_encode': 'sentiment_remapped', 'mapping':target_mapping})),\n",
    "])\n",
    "\n",
    "df = df_preprocess.fit_transform(df) #\n",
    "df\n",
    "# Este debiese ser la primera transformación, no se si serializar esto o no pero habría que indicar que es necesario aplicarlo a df para poder continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos los pipelines siguientes deberían vivir en un solo pipeline y al final aplicar fit_transform. Se debe serializar ese ultimo pipeline\n",
    "\n",
    "\n",
    "# hiperparametros posibles:\n",
    "# usar o no STOPWORDS\n",
    "# stemmers = 'ps' | 'wnl' | 'sno'\n",
    "\n",
    "preprocessing = Pipeline(steps=[\n",
    "                ('rc', FunctionTransformer(utils.remove_chars, kw_args={'var_name': 'content', 'char_list': chars_to_replace})),\n",
    "                ('ra', FunctionTransformer(utils.remove_arrobas, kw_args= {'var_name':'content_remchars'})),\n",
    "                ('rl', FunctionTransformer(utils.remove_links, kw_args={'var_name': 'content_remchars_remarroba'})),\n",
    "                ('rsw', RemoveStopWords(text_columns = ['content_remchars_remarroba_remlinks'])),\n",
    "                ('lt', LemmantizerTransformer(text_columns = ['content_remchars_remarroba_remlinks_sw'], stemmers=['ps'])),\n",
    "                \n",
    "])\n",
    "\n",
    "# hiperparametros posibles:\n",
    "# usar o no STOPWORDS\n",
    "# min_df = 1,2,3...\n",
    "# vect_type = 'count' | 'tfid'\n",
    "\n",
    "# Si se quiere ver los resultados de las limpiezas de los textos, comentar la linea de ColumnSelectedTransformer y luego ver el dataframe. También se pueden ver los reportes generados en csv y txt en la carpeta raíz.\n",
    "\n",
    "feature_extraction = Pipeline(steps=[\n",
    "                ('fet_1', FeatureExtractionTwitts(\n",
    "                    text_column=\"content\",\n",
    "                    features_to_extract=[\n",
    "                        \"arrobas_count\", \"hashtag_count\", \"is_reply\", \"is_rt\", \"twitt_length\"]\n",
    "                    )),\n",
    "                ('fet_2', FeatureExtractionTwitts(\n",
    "                    text_column=\"content_remchars_remarroba_remlinks_sw_ps\",\n",
    "                    features_to_extract = [\"subjectivity\", \"polarity\"]\n",
    "                    )),\n",
    "                ('ctr', Vectorizer(vect_type='count', text_column='content_remchars_remarroba_remlinks_sw_ps', min_df = 10)),\n",
    "                ('ds', ColumnSelectedTransformer(vars_prefix='var_'))\n",
    "                # ('reorder', FunctionTransformer(utils.columns_reorder, kw_args={'new_columns_ordered': new_ordered_columns})),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# new_ordered_columns = df.columns[df.columns.str.contains('content')].to_list(\n",
    "# ) + df.columns[df.columns.str.contains('var')].to_list()  + df.columns[df.columns.str.contains('sentiment')].to_list()\n",
    "\n",
    "# joblib.load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "def train_function(pipe, X_train, X_test, y_train, y_test):\n",
    "    X_train, y_train = pipe.fit_transform(X_train, y_train)\n",
    "    X_test, y_test = pipe.transform(X_test, y_test)\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print('train')\n",
    "    print(classification_report(y_train, y_pred_train, digits=4))\n",
    "    print('test')\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return pipe\n",
    "\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "clf4 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "meta0 = LogisticRegression(random_state=42)\n",
    "meta1 = RandomForestClassifier(random_state=42)\n",
    "meta2 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "estimators = [('gnb', clf1), ('knn', clf4)]\n",
    "\n",
    "\n",
    "# final estimator: metaestimator\n",
    "st_1 = StackingClassifier(estimators=estimators,\n",
    "                        final_estimator=meta0,\n",
    "                        cv=5)\n",
    "\n",
    "model_and_scale = Pipeline(steps=[\n",
    "    ('sc', StandardScaler()),\n",
    "    ('model', st_1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomas\\Desktop\\Tareas Data Science - Desafío Latam\\machine_learning\\prueba\\twitter\\twitter.ipynb Celda 9\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msentiment_remapped\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_function(pipe, X_train, X_test, y_train, y_test)\n",
      "\u001b[1;32mc:\\Users\\tomas\\Desktop\\Tareas Data Science - Desafío Latam\\machine_learning\\prueba\\twitter\\twitter.ipynb Celda 9\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(pipe, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_function\u001b[39m(pipe, X_train, X_test, y_train, y_test):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     X_train, y_train \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mfit_transform(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     X_test, y_test \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mtransform(X_test, y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/twitter/twitter.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y_pred_train \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\pipeline.py:422\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\pipeline.py:422\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:581\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    560\u001b[0m     \u001b[39m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \n\u001b[0;32m    562\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39m        Returns a fitted instance of estimator.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     check_classification_targets(y)\n\u001b[0;32m    582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit(y)\n\u001b[0;32m    583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le\u001b[39m.\u001b[39mclasses_\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\utils\\multiclass.py:200\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    192\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[0;32m    194\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m ]:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    # ('model', model_and_scale)\n",
    "])\n",
    "\n",
    "X = df[['content']]\n",
    "y = df.sentiment_remapped\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "train_function(pipe, X_train, X_test, y_train, y_test);\n",
    "# pipe.fit_transform(X_train, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(X_tr['var_subjectivity'], bins=50, fc=(1, 0, 0, 0.5))\n",
    "plt.hist(X_tr['var_polarity'], bins=50, fc=(0, 0, 1, 0.5))\n",
    "plt.title('Sentiment analysis')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(X_tr['var_twit_length'], bins=30, fc=(1, 0, 0, 0.5))\n",
    "plt.title('Twits length')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9ccd8639d7ac6d8ae46f08631d02de0d1c9f4a08850208985333be71082afd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
