{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "# ------------------------------------------\n",
    "import nltk\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "## PROPIAS\n",
    "from utils import RemoveStopWords\n",
    "from utils import FeatureExtractionTwitts\n",
    "from utils import LemmantizerTransformer\n",
    "from utils import Vectorizer\n",
    "from utils import ColumnSelectedTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from utils import KerasCustomClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_tweets.csv', index_col = 0)\n",
    "df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content.to_csv('raw_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_classes = {\n",
    "                    'happiness' : 'positiva',\n",
    "                    'surprise'  : 'positiva',\n",
    "                    'love'      : 'positiva',\n",
    "                    'fun'       : 'positiva',\n",
    "                    'relief'    : 'positiva',\n",
    "                    'enthusiasm': 'positiva',\n",
    "                    'worry'     : 'negativa',\n",
    "                    'hate'      : 'negativa',\n",
    "                    'sadness'   : 'negativa',\n",
    "                    'empty'     : 'negativa',\n",
    "                    'boredom'   : 'negativa',\n",
    "                    'anger'     : 'negativa',\n",
    "                    'neutral'   : 'neutral'\n",
    "                }\n",
    "neutral_class_name = 'neutral'\n",
    "target_var_name = 'sentiment'\n",
    "target_mapping = [('positiva',1),('negativa',0)]\n",
    "chars_to_replace = [\n",
    "                        ('ï¿½',''),\n",
    "                        ('&quot;',''),\n",
    "                        ('&lt;3','<3'), \n",
    "                        ('&lt;/3','</3'), \n",
    "                        ('&amp;','&'),\n",
    "                        ('&gt;','>'),\n",
    "                        ('&lt;','<')\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_remapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34723</th>\n",
       "      <td>Happy Mama's day to all mothers</td>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17493</th>\n",
       "      <td>@LysdelTellez I am lost. Please help me find a...</td>\n",
       "      <td>worry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20198</th>\n",
       "      <td>@BoomKatt yes yes I AM, networking whore to th...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>you@snapplynn Wish that would have been your t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>now i am doing the MicroEconomics project  iha...</td>\n",
       "      <td>worry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>I  do not want to work tomorrow!</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36006</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its s...</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22647</th>\n",
       "      <td>Photo: Got my prints a few days ago, ready for...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21478</th>\n",
       "      <td>@tove_liden Thanks for the follow Tove!</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39364</th>\n",
       "      <td>@esmeeworld thanks</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  sentiment  \\\n",
       "34723                    Happy Mama's day to all mothers       love   \n",
       "17493  @LysdelTellez I am lost. Please help me find a...      worry   \n",
       "20198  @BoomKatt yes yes I AM, networking whore to th...  happiness   \n",
       "6855   you@snapplynn Wish that would have been your t...    neutral   \n",
       "5924   now i am doing the MicroEconomics project  iha...      worry   \n",
       "...                                                  ...        ...   \n",
       "16469                   I  do not want to work tomorrow!    sadness   \n",
       "36006  @KandyBee we shuld do  a dance like that its s...        fun   \n",
       "22647  Photo: Got my prints a few days ago, ready for...  happiness   \n",
       "21478            @tove_liden Thanks for the follow Tove!        fun   \n",
       "39364                                 @esmeeworld thanks    neutral   \n",
       "\n",
       "       sentiment_remapped  \n",
       "34723                   1  \n",
       "17493                   0  \n",
       "20198                   1  \n",
       "6855                    0  \n",
       "5924                    0  \n",
       "...                   ...  \n",
       "16469                   0  \n",
       "36006                   1  \n",
       "22647                   1  \n",
       "21478                   1  \n",
       "39364                   1  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocess = Pipeline(steps=[\n",
    "                ('regroup_classes', \n",
    "                    FunctionTransformer(\n",
    "                        utils.multi_class_remapping, \n",
    "                        kw_args={\n",
    "                            'group_classes': groups_classes,\n",
    "                            'var_name': target_var_name,\n",
    "                            'neutral_class': neutral_class_name,\n",
    "                            'random_state': 42\n",
    "                            })),\n",
    "                ('encoding', FunctionTransformer(\n",
    "                        utils.target_encoding,\n",
    "                        kw_args={\n",
    "                            'column_to_encode': 'sentiment_remapped', 'mapping':target_mapping})),\n",
    "])\n",
    "\n",
    "df = df_preprocess.fit_transform(df) #\n",
    "df\n",
    "# Este debiese ser la primera transformación, no se si serializar esto o no pero habría que indicar que es necesario aplicarlo a df para poder continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['content']]\n",
    "y = df.sentiment_remapped\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(steps=[\n",
    "                ('rc', FunctionTransformer(utils.remove_chars, kw_args={'var_name': 'content', 'new_var_name': 'content_mod', 'char_list': chars_to_replace})),\n",
    "                ('ra', FunctionTransformer(utils.remove_arrobas, kw_args= {'var_name':'content_mod', 'new_var_name': 'content_mod'})),\n",
    "                ('rl', FunctionTransformer(utils.remove_links, kw_args={'var_name': 'content_mod', 'new_var_name': 'content_mod'})),\n",
    "                ('rsw', RemoveStopWords(text_columns = ['content_mod'])),\n",
    "                ('lt', LemmantizerTransformer(text_columns = [\"content_mod\"], stemmer='sno')),\n",
    "])\n",
    "\n",
    "# Si se quiere ver los resultados de las limpiezas de los textos, comentar la linea de ColumnSelectedTransformer y luego ver el dataframe. También se pueden ver los reportes generados en csv y txt en la carpeta raíz.\n",
    "\n",
    "feature_extraction = Pipeline(steps=[\n",
    "                ('fet_1', FeatureExtractionTwitts(text_column=\"content\",features_to_extract=[\"arrobas_count\", \"hashtag_count\", \"is_reply\", \"is_rt\", \"twitt_length\"])),\n",
    "                ('fet_2', FeatureExtractionTwitts(text_column=\"content_mod\",features_to_extract = [\"subjectivity\", \"polarity\"])),\n",
    "                ('vec', Vectorizer(vect_type='tfid', text_column=\"content_mod\", min_df=15, max_df=0.7, ngram_range=(1,3))),\n",
    "                ('fs', ColumnSelectedTransformer(vars_prefix='var_'))\n",
    "                \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de Clasificadores Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb     = MultinomialNB()\n",
    "gnb     = GaussianNB() # 59 seg train\n",
    "knc     = KNeighborsClassifier()\n",
    "svc     = SVC(random_state=42, probability=True)\n",
    "gbc     = GradientBoostingClassifier(random_state=42)\n",
    "# funciona particularmente bien stackeando xgboost\n",
    "lr      = LogisticRegression(random_state=42, C=0.01) # gnb\n",
    "rfc     = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10) # 61 train\n",
    "hgnb    = HistGradientBoostingClassifier(random_state=42)  # 1:43 min train\n",
    "\n",
    "# metaestimator stacking\n",
    "sc_1 = StackingClassifier(estimators=[lr, rfc], final_estimator=gbc, cv=5)\n",
    "# En este caso se eligieron estimadores de naturaleza distinta en los estimadores. En el final estimator se selecciono un gradient boosting.\n",
    "\n",
    "# metaestimator voting\n",
    "vc_1 = VotingClassifier([(\"lr\", lr), (\"knc\", knc), (\"gbc\", gbc)], voting=\"hard\", n_jobs=-1)\n",
    "# Se eligio un número impar de modelos para que siempre exista mayoría. \n",
    "\n",
    "# metaestimator boosting\n",
    "bc_1 = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5), random_state=42, n_estimators=100, learning_rate=1)\n",
    "\n",
    "nn_arch = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .2),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "kcc = KerasCustomClassifier(    \n",
    "                                nn_arch, \n",
    "                                loss='binary_crossentropy',\n",
    "                                optimizer='Adam',\n",
    "                                metrics='accuracy',\n",
    "                                net_name='twitter_keras_net',\n",
    "                                epochs=10\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimización del sub-pipeline de preprocessing y feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'preprocessing__rsw': [RemoveStopWords(text_columns = ['content_mod']), None],\n",
    "    'preprocessing__lt': [LemmantizerTransformer(text_columns = [\"content_mod\"]), None],\n",
    "    'preprocessing__lt__stemmer': ['sno', 'ps', 'wnl'],\n",
    "    'feature_extraction__vec__vect_type': ['tfid'],\n",
    "    'feature_extraction__vec__min_df': [5, 10, 15]\n",
    "}\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', rfc)\n",
    "])\n",
    "\n",
    "search = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=4, verbose=10)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search.best_params_, 'best_score': search.best_score_, 'results': pd.DataFrame(\n",
    "    search.cv_results_), 'params': params}, 'gridsearch_preprocessing_twitter.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 1: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', sc_1)\n",
    "])\n",
    "\n",
    "params_stacking = {\n",
    "                    'model__n_estimators': [50, 100, 500],\n",
    "                    'model__max_depth': [5, 10, 50]\n",
    "                }\n",
    "\n",
    "search_stacking = GridSearchCV(pipe_stacking, params_stacking, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_stacking.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search.best_params_, 'best_score': search.best_score_, 'results': pd.DataFrame(search.cv_results_), 'params': params}, 'gridsearch_stacking_twitter.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 2: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_voting = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', gbc)\n",
    "])\n",
    "\n",
    "params_voting = {\n",
    "                    'model__voting': ['hard','soft'],\n",
    "                    'model__learning_rate' : []\n",
    "                }\n",
    "\n",
    "search_voting = GridSearchCV(pipe_voting, params_voting, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "search_voting.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_voting.best_params_, 'best_score': search_voting.best_score_, 'results': pd.DataFrame(search_voting.cv_results_), 'params': params_voting}, 'gridsearch_voting_twitter.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 3: GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gboost = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', vc_1)\n",
    "])\n",
    "\n",
    "params_gboost = {\n",
    "                    'model__n_estimators':[10,50, 100, 200],\n",
    "                    'model__learning_rate':[0.01, 0.1, 0.5, 1],\n",
    "                    'model__max_depth':[1,2,3,4,5]\n",
    "                }\n",
    "\n",
    "search_gboost = GridSearchCV(pipe_gboost, params_gboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_gboost.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_gboost.best_params_, 'best_score': search_gboost.best_score_, 'results': pd.DataFrame(search_gboost.cv_results_), 'params': params_gboost}, 'gridsearch_gboost_twitter.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 4: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_adaboost = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', bc_1)\n",
    "])\n",
    "\n",
    "params_adaboost = {\n",
    "                    'model__n_estimators':[10,50, 100, 200],\n",
    "                    'model__learning_rate':[0.01, 0.1, 0.5, 1]\n",
    "                }\n",
    "\n",
    "search_adaboost = GridSearchCV(pipe_adaboost, params_adaboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_adaboost.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_adaboost.best_params_, 'best_score': search_adaboost.best_score_, 'results': pd.DataFrame(search_adaboost.cv_results_), 'params': params_adaboost}, 'gridsearch_adaboost_twitter.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 5: Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nn = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_extraction', feature_extraction),\n",
    "    ('model', kcc)\n",
    "])\n",
    "\n",
    "nn_arch_1 = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .2),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_2 = {\n",
    "    'input_layer'       : ('input_dense', 16, 'relu'),\n",
    "    'drop_1'            : ('dropout', .2),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_3 = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .1),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_4 = {\n",
    "    'input_layer'       : ('input_dense', 16, 'relu'),\n",
    "    'drop_1'            : ('dropout', .1),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_5 = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .4),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_6 = {\n",
    "    'input_layer'       : ('input_dense', 16, 'relu'),\n",
    "    'drop_1'            : ('dropout', .4),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "params_nn = {   \n",
    "                'model__epochs': [5, 10, 20],\n",
    "                'model__nn_arch':[nn_arch_1, nn_arch_2, nn_arch_3],\n",
    "            }\n",
    "\n",
    "search_nn = GridSearchCV(pipe_nn, params_nn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_nn.fit(X_train, y_train)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_nn.best_params_, 'best_score': search_nn.best_score_,\n",
    "                          'results': pd.DataFrame(search_nn.cv_results_), 'params': params_nn}, 'gridsearch_nn_twitter.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialización de los 2 mejores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe_1 = Pipeline(steps=[\n",
    "                                ('preprocessing', preprocessing),\n",
    "                                ('feature_extraction', feature_extraction),\n",
    "                                ('model', GradientBoosting(parametros_optimos))\n",
    "                            ])\n",
    "\n",
    "best_pipe_2 = Pipeline(steps=[\n",
    "                                ('preprocessing', preprocessing),\n",
    "                                ('feature_extraction', feature_extraction),\n",
    "                                ('model', GradientBoosting(parametros_optimos))\n",
    "                            ])\n",
    "\n",
    "best_pipe_1 = utils.train_function(best_pipe_1, X, y)\n",
    "best_pipe_2 = utils.train_function(best_pipe_2, X, y)\n",
    "\n",
    "utils.save_bytes_variable({'best_pipe_1': best_pipe_1, 'best_pipe_2': best_pipe_2, 'target_process': df_preprocess}, 'twitter_best_models.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "nombre_archivo = 'twitter_best_models.pickle'\n",
    "dct = utils.load_bytes_variable(nombre_archivo)\n",
    "\n",
    "df_twitter_test = pd.read_csv('twitter_test_set.csv', index_col=0) # cambiar nombre csv por el del archivo test\n",
    "\n",
    "df_twitter_test = dct['df_preprocess'].transform(df_twitter_test)\n",
    "\n",
    "X = df_twitter_test[['content']]\n",
    "y = df_twitter_test.sentiment_remapped\n",
    "\n",
    "utils.test_function(dct['best_pipe_1'], X, y) # Predicciones modelo 1\n",
    "utils.test_function(dct['best_pipe_2'], X, y) # Predicciones modelo 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9ccd8639d7ac6d8ae46f08631d02de0d1c9f4a08850208985333be71082afd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
