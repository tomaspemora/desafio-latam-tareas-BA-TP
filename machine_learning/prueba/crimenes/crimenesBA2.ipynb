{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Machine Learning Prueba 2 - Analizando los crímenes en la Ciudad de Nueva York***.\n",
    "### Nombre(s): Thomas Peet, Braulio Águila, Camilo Ramírez\n",
    "### Generación: G47\n",
    "### Profesores: Alfonso Tobar - Sebastián Ulloa\n",
    "### Fecha: 06-10-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Contexto*\n",
    "En esta ocasión trabajaremos con datos públicos del departamento de policía de New York.    \n",
    "El dataset es llamado stop_and_frisk_data y contiene información sobre interrogaciones    \n",
    "y detenciones realizadas por el departamento de policía de NY en la vía pública. El    \n",
    "diccionario de atributos se encuentra en el archivo 2009 SQF File Spec.xls.    \n",
    "Para todo nuestro estudio utilizaremos los datos correspondientes al año 2009 como    \n",
    "conjunto de entrenamiento y los datos del 2010 como conjunto de pruebas. Hay que hacer    \n",
    "notar que los datos que estamos utilizando son un muestreo del de la cantidad de registros    \n",
    "reales que contiene el dataset, esta decisión fue tomada debido a los largos tiempos de    \n",
    "entrenamiento y procesamiento que requiere el volumen de datos reales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Objetivos*\n",
    "Para alcanzar el objetivo general, su trabajo se puede desagregar en los siguientes puntos:  \n",
    "\n",
    "1. Debe analizar de forma exploratoria los atributos. Reporte la cantidad de datos    \n",
    "perdidos y presente su esquema de recodificación. Tenga presente que lo que    \n",
    "observe en el análisis exploratorio debe guiar su proceso de ingeniería de atributos,    \n",
    "por lo que se le recomienda que piense en aspectos de las variables involucradas     \n",
    "que puedan afectar el proceso mencionado.\n",
    "\n",
    "2. Reporte la probabilidad de que un individuo sea arrestado en uno de los cinco  \n",
    "barrios, condicional al género y a la raza. Concluya, ¿qué implicancias éticas tienen  \n",
    "algunas conclusiones de lo que observa?.\n",
    "\n",
    "3. Entregue un modelo predictivo que prediga efectivamente si un determinado  \n",
    "procedimiento concluirá en un arresto o no. Para ello, guíate por los siguientes  \n",
    "lineamientos:  \n",
    "    - Entrene por lo menos 3 modelos que sean capaces de predecir si se  \n",
    "producirá un arresto o no. Una vez que encuentre un modelo satisfactorio,  \n",
    "reporte al menos dos métricas de desempeño.  \n",
    "    - Refine aquellos atributos relevantes con alguna estrategia que crea  \n",
    "conveniente y reporte por lo menos 5 atributos relevantes para realizar la  \n",
    "predicción.\n",
    "\n",
    "4. Genere al menos cinco modelos predictivos que permitan determinar si el  \n",
    "procedimiento policial concluirá en alguna acción violenta.  \n",
    "○ Para ello, debe generar un nuevo atributo como vector objetivo que indique  \n",
    "cuándo hubo violencia o no. Éste debe ser creado a partir de atributos  \n",
    "existentes que indiquen el tipo de violencia.\n",
    "  \n",
    "5. Seleccione los 2 mejores modelos, serialicelos y envíalos a evaluación. Recuerde que  \n",
    "el modelo serializado debe ser posterior al fit, para poder ejecutar predict en los  \n",
    "nuevos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Tipo de problema a resolver:\n",
    "- De acuerdo con el enunciado y una revisión preliminar de los datos entregados, ambas problemáticas planteadas,  \n",
    "el hecho de que ocurra o no un arresto, y de que un procedimiento policial es o no violento, corresponden a   \n",
    "problemas de **clasificación**, ya que ambas variables objetivos son discretas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Tipo de métricas a implementar:\n",
    "- Las métricas que se utilizarán para la división de muestras corresponden a :\n",
    "- Tipo de preprocesamiento: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Modelos (5) con gridsearch e hiperparamétros tentativos/definitivos:\n",
    "- Modelo 1:\n",
    "- Modelo 2:\n",
    "- Modelo 3:\n",
    "- Modelo 4:\n",
    "- Modelo 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Comportamiento de variables objetivo (recodificados):\n",
    "- Variable objetivo 1: procedimiento policial en el que ocurre o no un arresto (\"arstmade\")\n",
    "- Variable objetivo 2: procedimiento policial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "#from feature engine drop features ----> sacar features para el pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from pathlib import Path\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "#import shapely\n",
    "#import folium \n",
    "import pyproj\n",
    "import helpers as hp\n",
    "import preproc_nyc_sqf_V2 as preproc\n",
    "import contextily as cx\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import CreateSuitableDataframeTransformer\n",
    "from utils import OrdinalEncoderFixedTransformer\n",
    "from utils import DropRowsTransformer\n",
    "from utils import CriterioExperto\n",
    "from utils import KerasCustomClassifier\n",
    "import utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Dataset interrogaciones/detenciones Policía de Nueva York - 2009***:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de los datasets para los 2 problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se eliminan los datos sin xcoord o ycoord\n",
    "* Se eliminan los registros de detenidos menores de 18 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2009 = pd.read_csv('2009_1perc.csv', index_col=0)\n",
    "\n",
    "drt = DropRowsTransformer()\n",
    "df2009_nonull = drt.transform(df2009)\n",
    "\n",
    "# Explicar que esta función segmenta el dataframe en 2 problemas: clasificación de arrestos y clasificación de arrestos con violencia.\n",
    "# Exploración variable objetivo 2 (\"violencia física en arresto\") --- Necesita ser recodificada\n",
    "# El mismo dataframe aplicando los criterios de descartar datos sin xcoord/ycoord y registros de menores de 18 años\n",
    "x_1, y_1, x_2, y_2 = utils.split_features_target(df2009_nonull)\n",
    "x_1.shape, y_1.shape, x_2.shape, y_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Exploración variable objetivo 1 (\"arstmade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de el vector objetivo:\n",
    "sns.set(font_scale=2)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(y_1); # Categoría uno muchos menos casos que categoría 0.\n",
    "plt.title(\"Vector objetivo : arstmade\")\n",
    "plt.xlabel(\"clases\");\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(y_2); # Categoría uno muchos menos casos que categoría 0.\n",
    "plt.title(\"Vector objetivo : violence\")\n",
    "plt.xlabel(\"clases\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteo de nulos\n",
    "#proc_df.info(verbose=True, show_counts=True,memory_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.cat_num_rate_analysis(df2009_nonull);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Analisis de correlación para feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import PrintVars\n",
    "# def transform_pipeline():\n",
    "#     pipe = Imb_Pipeline(steps=[\n",
    "#         # ('csd', CreateSuitableDataframeTransformer()),\n",
    "#         # ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "#         # ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "#         # ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered'))\n",
    "\n",
    "#         ('csd', CreateSuitableDataframeTransformer()),\n",
    "#         ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "#         ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "#         ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "#         ('smote', SMOTE(sampling_strategy='auto',random_state=42)),\n",
    "#         ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "#     ])\n",
    "#     return pipe\n",
    "\n",
    "# pipex = transform_pipeline()\n",
    "# df2009_nonull_tr = pipex.fit_transform(df2009_nonull.drop(columns = 'arstmade'), y_1)\n",
    "# utils.get_top_correlations_blog(df2009_nonull_tr, threshold=0.6)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# display(df2009_nonull_tr.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de columnas eliminadas en base a \"Criterio Expertos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerar variables relevantes para modelos de procedimiento que concluye en arresto y/o procedimiento que concluye en situación violenta:\n",
    "# redefinir variable \"others\" (esta mál escrita con respecto a la planilla)\n",
    "\n",
    "var_eliminar_pf = [\"pf_baton\", \"pf_hcuff\", \"pf_pepsp\", \"pf_other\", \"pf_ptwep\", \"pf_drwep\", \"pf_wall\", \"pf_hands\", \"pf_grnd\"] # 9 variables eliminadas \n",
    "\n",
    "var_eliminar_por_corr = [\"searched\", \"offunif\", \"offverb\", \"frisked\", \"rf_bulg\"]\n",
    "\n",
    "var_eliminar_others = [\"ac_rept\", \"ac_inves\", \"ac_proxm\", \"cs_casng\", \"cs_lkout\", \"explnstp\",\"sumissue\", \"offunif\", \"officrid\", \"frisked\", \"cs_cloth\", \"offverb\", \"rf_furt\", \"ac_other\", \"rf_bulg\", \"cs_furtv\", \"recstat\", \"cs_bulge\", \"cs_other\", \"trhsloc\", \"build\", \"beat\", \"post\",\"rf_attir\", \"cs_objcs\",\"eyecolor\", \"haircolr\", \"sector\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import PrintVars\n",
    "# def transform_pipeline():\n",
    "#     pipe = Imb_Pipeline(steps=[\n",
    "#         # ('csd', CreateSuitableDataframeTransformer()),\n",
    "#         # ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "#         # ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "#         # ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered'))\n",
    "#         ('ce', CriterioExperto(columns=var_eliminar_others + var_eliminar_pf)),\n",
    "#         ('csd', CreateSuitableDataframeTransformer()),\n",
    "#         ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "#         ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "#         ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "#         ('smote', SMOTE(sampling_strategy='auto',random_state=42)),\n",
    "#         ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "#     ])\n",
    "#     return pipe\n",
    "\n",
    "# pipex = transform_pipeline()\n",
    "# df2009_nonull_tr = pipex.fit_transform(df2009_nonull.drop(columns = 'arstmade'), y_1)\n",
    "# # utils.get_top_correlations_blog(df2009_nonull_tr, threshold=0.6)\n",
    "\n",
    "# # pd.set_option('display.max_rows', None)\n",
    "# # pd.set_option('display.max_columns', None)\n",
    "\n",
    "# display(df2009_nonull_tr.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de clasificadores base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se utiliza función entregada para limpieza preliminar de la data, se obtiene un df nuevo procesado y dos listas:\n",
    "# proc_df #este nuevo dataframe, estas constituido de 69 variables categóricas y 2 númericas sintéticas (\"meters\", \"month\")\n",
    "\n",
    "gnb = GaussianNB()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = SVC(random_state=42, probability=True)\n",
    "# funciona particularmente bien stackeando xgboost\n",
    "lr = LogisticRegression(random_state=42)\n",
    "rfc = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición del main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solución para usar SMOTE en gridsearch con muchos pasos: https://stackoverflow.com/questions/65652054/not-able-to-feed-the-combined-smote-randomundersampler-pipeline-into-the-main\n",
    "\n",
    "from utils import PrintVars\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def main_pipeline(estimator, type='accuracy'):\n",
    "    if type=='accuracy':\n",
    "        return Imb_Pipeline(steps=[\n",
    "            # ('ce', CriterioExperto(columns = var_eliminar_others + var_eliminar_pf)),\n",
    "            ('ce', FunctionTransformer(utils.criterio_experto, kw_args={'columns': var_eliminar_por_corr + var_eliminar_pf})),\n",
    "            ('csd', CreateSuitableDataframeTransformer()),\n",
    "            ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "            ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "            ('num_imp', MeanMedianImputer(imputation_method='median')),\n",
    "            ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "            ('model', estimator)\n",
    "        ])\n",
    "    \n",
    "    return Imb_Pipeline(steps=[\n",
    "        ('csd', CreateSuitableDataframeTransformer()),\n",
    "        ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "        # aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "        ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),\n",
    "        ('num_imp', MeanMedianImputer(imputation_method='median')),\n",
    "        ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "        ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "\n",
    "def gridsearch_train_and_save(pipe, params, scoring='accuracy', file_name='file.pickle'):\n",
    "    search = GridSearchCV(pipe, params, cv=5, scoring=scoring, n_jobs=4, error_score='raise', verbose=10)\n",
    "    search.fit(x_1, y_1)\n",
    "\n",
    "    utils.save_bytes_variable({'best_params': search.best_params_, 'best_score': search.best_score_, 'pipe': pd.DataFrame(search.cv_results_), 'params': params}, file_name)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimización del sub-pipeline de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(set(var_eliminar_por_corr + var_eliminar_others + var_eliminar_pf))\n",
    "l2 = list(set(var_eliminar_por_corr + var_eliminar_pf))\n",
    "l3 = list(set(var_eliminar_por_corr + var_eliminar_others))\n",
    "params = {\n",
    "        'ce': [     \n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l1}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l2}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l3}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':[]}),        \n",
    "                ],\n",
    "        'smote': [SMOTE(sampling_strategy='auto',random_state=42), None],\n",
    "        'cat_imp__imputation_method': ['frequent', 'missing'],\n",
    "        'num_imp__imputation_method': ['median', 'mean'],\n",
    "        'oe__encoding_method': ['ordered', 'arbitrary']\n",
    "}\n",
    "\n",
    "pipe1 = main_pipeline(rfc)\n",
    "pipe2 = main_pipeline(rfc)\n",
    "\n",
    "search_acc = gridsearch_train_and_save(pipe1, params, scoring='accuracy', file_name='gridsearch_preprocessing_crimenes_accuracy.pickle')\n",
    "search_rec = gridsearch_train_and_save(pipe2, params, scoring='recall', file_name='gridsearch_preprocessing_crimenes_recall.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_acc.best_params_, search_acc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rec.best_params_, search_rec.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Análisis y gráficos de probabilidades condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "df_probs = df2009_nonull.copy()\n",
    "var_pf = df_probs.columns[np.where([i[0:2]=='pf' for i in df_probs.columns.tolist()])].tolist()\n",
    "serie = pd.Series([int(np.isin([\"Y\"], df_probs[var_pf].iloc[i].values.tolist())[0]) for i in range(0, len(df_probs[var_pf]))], index = df_probs.index)\n",
    "# df_probs['violence'] = \n",
    "df_probs['violence'] = serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=True))\n",
    "counts = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "probs_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=True))\n",
    "counts_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "# Sex: F, M, Z\n",
    "# Race: A, B, I, P, Q, U, W, X, Z\n",
    "# City: BRONX, QUEENS, STATEN ISLAND, MANHATTAN, BROOKLYN\n",
    "\n",
    "zprobs = list(zip(probs['arstmade'], probs_v['violence'], probs.index))\n",
    "for x, y, i in zprobs:\n",
    "    # display(i)\n",
    "    if i[3] == 'N':\n",
    "        if i[2] == 'M':\n",
    "            if i[0] == 'BRONX':\n",
    "                c = 'red'\n",
    "            elif i[0] == 'BROOKLYN':\n",
    "                c = 'blue'\n",
    "            elif i[0] == 'MANHATTAN':\n",
    "                c = 'green'\n",
    "            elif i[0] == 'QUEENS':\n",
    "                c = 'magenta'\n",
    "            else:\n",
    "                c = 'orange'\n",
    "            \n",
    "            if i[1] == 'W':\n",
    "                m = 's'\n",
    "                s = 40\n",
    "            else:\n",
    "                m = '*'\n",
    "                s = 20\n",
    "            # elif \n",
    "\n",
    "        \n",
    "            plt.scatter(1-x,1-y, c= c, marker=m, s=s)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('arstmade')\n",
    "plt.ylabel('violence')\n",
    "\n",
    "\n",
    "# print(probs.query(\"race == 'B' & sex == 'M' \"))\n",
    "# print(probs.query(\"race == 'W' & sex == 'M' \"))\n",
    "# print('--------------')\n",
    "# print(probs_v.query(\"race == 'B' & sex == 'M' \"))\n",
    "# print(probs_v.query(\"race == 'W' & sex == 'M' \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Gráficos georeferenciados de arrestos y violencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gdf = gpd.GeoDataFrame(df2009_nonull, geometry=gpd.points_from_xy(df2009_nonull.xcoord, df2009_nonull.ycoord))\n",
    "\n",
    "df = gpd.read_file(gpd.datasets.get_path(\"nybb\"))\n",
    "df_wm = df.to_crs(epsg=3857)\n",
    "\n",
    "ax = df.plot(figsize=(15,10), alpha=0.5, edgecolor = \"k\")\n",
    "cx.add_basemap(ax,crs=df.crs, zoom =11, source=cx.providers.Stamen.TonerLite)\n",
    "#cx.add_basemap(ax, source=cx.providers.Stamen.TonerLabels)\n",
    "gdf.plot(ax=ax, color= \"red\", edgecolor = \"black\", markersize = 4)\n",
    "\n",
    "\n",
    "#Incorporar al código de arriba para poder plotear variable objetivo:\n",
    "# fig,ax = plt.subplots(1, 1) #plt.figure(figsize=(10,8))\n",
    "# fig.set_size_inches(25, 25)\n",
    "# df2009map[df2009map.arstmade == \"N\"].plot(color=\"red\", ax=ax, markersize = 1)\n",
    "# df2009map[df2009map.arstmade == \"Y\"].plot(ax=ax,  markersize = 12)\n",
    "# plt.show()\n",
    "# función transformar x,y to lon, lat:\n",
    "def xy_to_latlon(x,y):\n",
    "    source_crs = 'epsg:2263' # Coordinate system of the file\n",
    "    target_crs = 'epsg:4326' # Global lat-lon coordinate system\n",
    "    polar_to_latlon = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    lat, lon = polar_to_latlon.transform(x,y)\n",
    "    return lon, lat\n",
    "df_shape_ny = gpd.read_file(gpd.datasets.get_path(\"nybb\")).to_crs(epsg=3857)\n",
    "df_shape_ny\n",
    "\n",
    "import folium\n",
    "m = folium.Map(location=[40.7127837, -74.0059413],control_scale=True, zoom_start=10,tiles='CartoDB positron')\n",
    "# base_map.add_to(f)\n",
    "for _, r in df_shape_ny.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j)\n",
    "    # folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "    geo_j.add_to(m)\n",
    "m\n",
    "geometrias = [list(x.exterior.coords) for x in df_shape_ny.geometry[0]] \n",
    "from scipy.spatial import ConvexHull\n",
    "from folium import plugins\n",
    "import folium\n",
    "\n",
    "f = folium.Figure(width=700, height=400)    \n",
    "base_map = folium.Map(location=[40.7127837, -74.0059413],control_scale=True, zoom_start=10,tiles='CartoDB positron')\n",
    "base_map.add_to(f)\n",
    "\n",
    "# folium.GeoJson(data=df_shape_ny[\"geometry\"]).add_to(base_map)\n",
    "\n",
    "colors = ['orange','yellow','red','blue','green']\n",
    "\n",
    "for _, r in df_shape_ny.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry'])\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,  style_function=lambda x: {'fillColor': 'orange'})\n",
    "    # folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "    geo_j.add_to(base_map)\n",
    "\n",
    "\n",
    "# for point,color in zip(df_shape_ny[\"geometry\"],colors):\n",
    "# \t# if color == 'orange' or True:\n",
    "# \t\t# lats, longs = xy_to_latlon([i[0] for i in point],[i[1] for i in point], polar_to_latlon)\n",
    "# \t\t# positions = [(y, x) for x, y in zip(lats, longs)]\n",
    "# \t\t# points_rev = [(y,x) for x,y in point]\n",
    "# \t\t# points_rev = [points_rev[i] for i in ConvexHull(points_rev).vertices]\n",
    "# \t\t# if color == 'orange':\n",
    "# \t\t\t# display(positions)\n",
    "# \t# sim_geo = gpd.GeoSeries(point).simplify(tolerance=0.001)\n",
    "# \t# geo_j = sim_geo.to_json()\n",
    "# \t# rrr = gpd.GeoSeries(point).simplify(tolerance=0.001).to_json()\n",
    "# \tfolium.GeoJson(data=pd.Series(point),  style_function=lambda feature: {\n",
    "#             'fillColor': color,\n",
    "#             'fillOpacity': 0.8,\n",
    "#         }).add_to(base_map)\n",
    "\t# geo_j = folium.GeoJson(data=geo_j,  style_function=lambda x: {'fillColor': 'orange'})\n",
    "\t# folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "\t# geo_j.add_to(base_map)\n",
    "\n",
    "\t# plugins.PolyLineOffset(locations=point, color='blue', fill=True, fill_color=color, fill_opacity=0.5, smooth_factor=.1).add_to(base_map)\n",
    "\n",
    "# lats, longs = xy_to_latlon(df2009.xcoord.to_list(), df2009.ycoord.to_list(), xy_to_latlon)\n",
    "# for lat, lon, i in zip(lats, longs, df2009.arstmade):\n",
    "#     color = 'blue' if i == 'Y' else '#FF000030'\n",
    "#     size = '20' if i == 'Y' else '10'\n",
    "#     border = 'none' if i == 'Y' else 'none'\n",
    "#     folium.Marker(location=[str(lon), str(lat)], icon=folium.DivIcon(html=f\"<span style='font-size:{size}px;color:{color};border:{border}'>&#9670;</span>\"),popup=i).add_to(base_map)\n",
    "base_map\n",
    "#otra opción de crear mapa con folium:\n",
    "import folium\n",
    "def generateBaseMap(loc, zoom=11, tiles='OpenStreetMap', crs='ESPG3857'):\n",
    "    return folium.Map(location=loc,\n",
    "                   #control_scale=True, \n",
    "                   zoom_start=zoom,\n",
    "                   #tiles=tiles)\n",
    "    )\n",
    "base_map = generateBaseMap([40.7127837, -74.0059413])\n",
    "\n",
    "\n",
    "marker = list(range(len(df2009.xcoord)))\n",
    "counter = 0\n",
    "tooltip = \"Click Here For More Info\"\n",
    "icon = folium.features.CustomIcon('https://cdn-icons-png.flaticon.com/128/7500/7500224.png', icon_size=(40, 40))\n",
    "\n",
    "for x,y in zip(df2009.xcoord, df2009.ycoord):\n",
    "    lon, lat = xy_to_latlon(x,y)\n",
    "    marker[counter] = folium.Marker(icon=icon,\n",
    "    #     #location=[40.7127837, -74.0059413],\n",
    "    location=[lon, lat],\n",
    "    #     #popup=\"<stong>Allianz Arena</stong>\",\n",
    "    #     #tooltip=tooltip\n",
    "    )\n",
    "    marker[counter].add_to(base_map)\n",
    "    #print(f\"latitud {lat} y longitud {lon}\")\n",
    "    if counter>5 : \n",
    "        break\n",
    "    counter += 1\n",
    "base_map\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de Clasificadores Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb     = GaussianNB() # 59 seg train\n",
    "knc     = KNeighborsClassifier()\n",
    "svc     = SVC(random_state=42, probability=True)\n",
    "gbc     = GradientBoostingClassifier(random_state=42)\n",
    "# funciona particularmente bien stackeando xgboost\n",
    "lr      = LogisticRegression(random_state=42, C=0.01) # gnb\n",
    "rfc     = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10) # 61 train\n",
    "\n",
    "# metaestimator stacking\n",
    "sc_1 = StackingClassifier(estimators=[(\"lr\", lr), (\"rfc\", rfc)], final_estimator=gbc, cv=5)\n",
    "# En este caso se eligieron estimadores de naturaleza distinta en los estimadores. En el final estimator se selecciono un gradient boosting.\n",
    "\n",
    "# metaestimator voting\n",
    "vc_1 = VotingClassifier([(\"lr\", lr), (\"knc\", knc), (\"gbc\", gbc)], voting=\"hard\", n_jobs=-1)\n",
    "# Se eligio un número impar de modelos para que siempre exista mayoría. \n",
    "\n",
    "# metaestimator boosting\n",
    "bc_1 = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5), random_state=42, n_estimators=100, learning_rate=1)\n",
    "\n",
    "nn_arch = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .2),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "kcc = KerasCustomClassifier(    \n",
    "                                nn_arch, \n",
    "                                loss='binary_crossentropy',\n",
    "                                optimizer='Adam',\n",
    "                                metrics='accuracy',\n",
    "                                net_name='twitter_keras_net',\n",
    "                                epochs=10\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 1: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking = main_pipeline(sc_1)\n",
    "\n",
    "params_stacking = {\n",
    "    'model__n_estimators': [50, 100, 500],\n",
    "    'model__max_depth': [5, 10, 50]\n",
    "}\n",
    "\n",
    "search_stacking = GridSearchCV(pipe_stacking, params_stacking, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_stacking.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_stacking.best_params_, 'best_score': search_stacking.best_score_, 'results': pd.DataFrame(\n",
    "    search_stacking.cv_results_), 'params': params}, 'gridsearch_stacking_crimenes.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 2: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_voting = main_pipeline(vc_1)\n",
    "\n",
    "params_voting = {\n",
    "                    'model__voting': ['hard','soft']\n",
    "                }\n",
    "\n",
    "search_voting = GridSearchCV(pipe_voting, params_voting, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "search_voting.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_voting.best_params_, 'best_score': search_voting.best_score_, 'results': pd.DataFrame(search_voting.cv_results_), 'params': params_voting}, 'gridsearch_voting_crimenes.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 3: GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gboost = main_pipeline(gbc)\n",
    "\n",
    "params_gboost = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'model__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "search_gboost = GridSearchCV(pipe_gboost, params_gboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_gboost.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_gboost.best_params_, 'best_score': search_gboost.best_score_, 'results': pd.DataFrame(\n",
    "    search_gboost.cv_results_), 'params': params_gboost}, 'gridsearch_gboost_crimenes.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 4: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_adaboost = main_pipeline(bc_1)\n",
    "\n",
    "params_adaboost = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "search_adaboost = GridSearchCV(\n",
    "    pipe_adaboost, params_adaboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_adaboost.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_adaboost.best_params_, 'best_score': search_adaboost.best_score_, 'results': pd.DataFrame(\n",
    "    search_adaboost.cv_results_), 'params': params_adaboost}, 'gridsearch_adaboost_crimenes.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GridSearch de Modelo 2: Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nn = main_pipeline(kcc)\n",
    "\n",
    "nn_arch_1 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_2 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_3 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_4 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_5 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_6 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "params_nn = {\n",
    "    'model__epochs': [5, 10, 20],\n",
    "    'model__nn_arch': [nn_arch_1, nn_arch_2, nn_arch_3, nn_arch_4, nn_arch_5, nn_arch_6],\n",
    "}\n",
    "\n",
    "search_nn = GridSearchCV(pipe_nn, params_nn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "search_nn.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_params': search_nn.best_params_, 'best_score': search_nn.best_score_, 'results': pd.DataFrame(search_nn.cv_results_), 'params': params_nn}, 'gridsearch_nn_crimenes.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propongo que los dos mejores los pasemos por un gridsearch recall y usemos ese como el modelo ganador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geopandas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cfc4558a56c74004f66639ccbe2b85f126442f2f38d8bdd913543c2f1bf5fd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
