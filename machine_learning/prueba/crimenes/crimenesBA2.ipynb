{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Machine Learning Prueba 2 - Analizando los crímenes en la Ciudad de Nueva York***.\n",
    "### Nombre(s): Thomas Peet, Braulio Águila, Camilo Ramírez\n",
    "### Generación: G47\n",
    "### Profesores: Alfonso Tobar - Sebastián Ulloa\n",
    "### Fecha: 14-10-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "#import shapely\n",
    "#import folium \n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "\n",
    "# Herramientas base\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "\n",
    "# Transformers importados\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Transformers customizados\n",
    "import utils\n",
    "from utils import CreateSuitableDataframeTransformer\n",
    "from utils import OrdinalEncoderFixedTransformer\n",
    "from utils import DropRowsTransformer\n",
    "from utils import PrintVars\n",
    "\n",
    "# Estimators importados desde librerías\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Estimators customizados\n",
    "from utils import KerasCustomClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lectura de Dataset interrogaciones/detenciones Policía de Nueva York - 2009***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2009 = pd.read_csv('2009_1perc.csv', index_col=0)\n",
    "df2009.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Limpieza del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta transformación (se serializa para ser aplicada también en testing) aplica los siguientes criterios para descartar datos\n",
    "- Datos sin xcoord/ycoord\n",
    "- Registros de menores de 18 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drt = DropRowsTransformer()\n",
    "df2009_nonull = drt.transform(df2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Definición de los 2 problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `split_features_target` segmenta el dataframe en 2 problemas: clasificación de arrestos y clasificación de arrestos con violencia.\n",
    "\n",
    "En esta exploración se definirá la variable y_1 como la columna 'arstmade' y la variable y_2 como una construcción a partir de las siguientes columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_eliminar_pf = [\"pf_baton\", \"pf_hcuff\", \"pf_pepsp\", \"pf_other\", \"pf_ptwep\", \"pf_drwep\", \"pf_wall\", \"pf_hands\", \"pf_grnd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera en que se construirá y_2 a partir de estas variables es a través una función AND entre todas ellas.\n",
    "\n",
    "En consecuencia estas variables se eliminan de la matriz de predictores del segundo problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1, y_1, x_2, y_2 = utils.split_features_target(df2009_nonull, var_eliminar_pf)\n",
    "x_1.shape, y_1.shape, x_2.shape, y_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploración de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tan categóricas son las variables? ¿ Si una variable se dice que es categórica, pero tiene casi tantas clases como datos, es correcto seguir considerandola como categórica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cat_num_rate_analysis(df2009_nonull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de distribución de targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploración de variable arstmade y variable violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de el vector objetivo:\n",
    "sns.set(font_scale=1.2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(y_1); # Categoría uno muchos menos casos que categoría 0.\n",
    "plt.title(\"Vector objetivo : arstmade\")\n",
    "plt.xlabel(\"clases\");\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(y_2);\n",
    "plt.title(\"Vector objetivo : violence\")\n",
    "plt.xlabel(\"clases\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de correlación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis de correlación para feature selection: Encodeamos los datos para poder calcular la matriz de correlación e identificar las variables que eliminaremos por tener una correlación superior a 0.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoderFixedTransformer(encoding_method='ordered')\n",
    "df2009_nonull_tr = oe.fit_transform(df2009_nonull.drop(columns = 'arstmade'), y_1)\n",
    "utils.get_top_correlations_blog(df2009_nonull_tr, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_eliminar_por_corr = [\"addrpct\", \"age_individual\",\"repcmd\", \"sumissue\",\"ht_feet\", \"searched\", \"offunif\", \"offverb\", \"frisked\", \"rf_bulg\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de columnas a eliminar en base a \"Criterios Expertos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerar variables relevantes para modelos de procedimiento que concluye en arresto y/o procedimiento que concluye en situación violenta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_eliminar_others = [\"ac_rept\", \"ac_inves\", \"ac_proxm\", \"cs_casng\", \"cs_lkout\", \"explnstp\",\"sumissue\", \"offunif\", \"officrid\", \"frisked\", \"cs_cloth\", \"offverb\", \"rf_furt\", \"ac_other\", \"rf_bulg\", \"cs_furtv\", \"recstat\", \"cs_bulge\", \"cs_other\", \"trhsloc\", \"build\", \"beat\", \"post\",\"rf_attir\", \"cs_objcs\",\"eyecolor\", \"haircolr\", \"sector\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis y gráficos de probabilidades condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "df_probs = df2009_nonull.copy()\n",
    "var_pf = df_probs.columns[np.where([i[0:2]=='pf' for i in df_probs.columns.tolist()])].tolist()\n",
    "serie = pd.Series([int(np.isin([\"Y\"], df_probs[var_pf].iloc[i].values.tolist())[0]) for i in range(0, len(df_probs[var_pf]))], index = df_probs.index)\n",
    "df_probs['violence'] = serie\n",
    "\n",
    "probs = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=True))\n",
    "counts = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "probs_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=True))\n",
    "counts_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "# Sex: F, M, Z\n",
    "# Race: A, B, I, P, Q, U, W, X, Z\n",
    "# City: BRONX, QUEENS, STATEN ISLAND, MANHATTAN, BROOKLYN\n",
    "\n",
    "zprobs = list(zip(probs['arstmade'], probs_v['violence'], probs.index))\n",
    "for x, y, i in zprobs:\n",
    "    # display(i)\n",
    "    if i[3] == 'N':\n",
    "        if i[2] == 'M':\n",
    "            if i[0] == 'BRONX':\n",
    "                c = 'red'\n",
    "            elif i[0] == 'BROOKLYN':\n",
    "                c = 'blue'\n",
    "            elif i[0] == 'MANHATTAN':\n",
    "                c = 'green'\n",
    "            elif i[0] == 'QUEENS':\n",
    "                c = 'magenta'\n",
    "            else:\n",
    "                c = 'orange'\n",
    "            \n",
    "            if i[1] == 'W':\n",
    "                m = 's'\n",
    "                s = 40\n",
    "            else:\n",
    "                m = '*'\n",
    "                s = 20\n",
    "            # elif \n",
    "\n",
    "        \n",
    "            plt.scatter(1-x,1-y, c= c, marker=m, s=s)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('arstmade')\n",
    "plt.ylabel('violence')\n",
    "\n",
    "\n",
    "# print(probs.query(\"race == 'B' & sex == 'M' \"))\n",
    "# print(probs.query(\"race == 'W' & sex == 'M' \"))\n",
    "# print('--------------')\n",
    "# print(probs_v.query(\"race == 'B' & sex == 'M' \"))\n",
    "# print(probs_v.query(\"race == 'W' & sex == 'M' \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentarios de las probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Gráficos georeferenciados de arrestos y violencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gdf = gpd.GeoDataFrame(df2009_nonull, geometry=gpd.points_from_xy(df2009_nonull.xcoord, df2009_nonull.ycoord))\n",
    "\n",
    "df = gpd.read_file(gpd.datasets.get_path(\"nybb\"))\n",
    "df_wm = df.to_crs(epsg=3857)\n",
    "\n",
    "ax = df.plot(figsize=(15,10), alpha=0.5, edgecolor = \"k\")\n",
    "cx.add_basemap(ax,crs=df.crs, zoom =11, source=cx.providers.Stamen.TonerLite)\n",
    "#cx.add_basemap(ax, source=cx.providers.Stamen.TonerLabels)\n",
    "gdf.plot(ax=ax, color= \"red\", edgecolor = \"black\", markersize = 4)\n",
    "\n",
    "\n",
    "#Incorporar al código de arriba para poder plotear variable objetivo:\n",
    "# fig,ax = plt.subplots(1, 1) #plt.figure(figsize=(10,8))\n",
    "# fig.set_size_inches(25, 25)\n",
    "# df2009map[df2009map.arstmade == \"N\"].plot(color=\"red\", ax=ax, markersize = 1)\n",
    "# df2009map[df2009map.arstmade == \"Y\"].plot(ax=ax,  markersize = 12)\n",
    "# plt.show()\n",
    "# función transformar x,y to lon, lat:\n",
    "def xy_to_latlon(x,y):\n",
    "    source_crs = 'epsg:2263' # Coordinate system of the file\n",
    "    target_crs = 'epsg:4326' # Global lat-lon coordinate system\n",
    "    polar_to_latlon = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    lat, lon = polar_to_latlon.transform(x,y)\n",
    "    return lon, lat\n",
    "df_shape_ny = gpd.read_file(gpd.datasets.get_path(\"nybb\")).to_crs(epsg=3857)\n",
    "df_shape_ny\n",
    "\n",
    "import folium\n",
    "m = folium.Map(location=[40.7127837, -74.0059413],control_scale=True, zoom_start=10,tiles='CartoDB positron')\n",
    "# base_map.add_to(f)\n",
    "for _, r in df_shape_ny.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j)\n",
    "    # folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "    geo_j.add_to(m)\n",
    "m\n",
    "geometrias = [list(x.exterior.coords) for x in df_shape_ny.geometry[0]] \n",
    "from scipy.spatial import ConvexHull\n",
    "from folium import plugins\n",
    "import folium\n",
    "\n",
    "f = folium.Figure(width=700, height=400)    \n",
    "base_map = folium.Map(location=[40.7127837, -74.0059413],control_scale=True, zoom_start=10,tiles='CartoDB positron')\n",
    "base_map.add_to(f)\n",
    "\n",
    "# folium.GeoJson(data=df_shape_ny[\"geometry\"]).add_to(base_map)\n",
    "\n",
    "colors = ['orange','yellow','red','blue','green']\n",
    "\n",
    "for _, r in df_shape_ny.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry'])\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,  style_function=lambda x: {'fillColor': 'orange'})\n",
    "    # folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "    geo_j.add_to(base_map)\n",
    "\n",
    "\n",
    "# for point,color in zip(df_shape_ny[\"geometry\"],colors):\n",
    "# \t# if color == 'orange' or True:\n",
    "# \t\t# lats, longs = xy_to_latlon([i[0] for i in point],[i[1] for i in point], polar_to_latlon)\n",
    "# \t\t# positions = [(y, x) for x, y in zip(lats, longs)]\n",
    "# \t\t# points_rev = [(y,x) for x,y in point]\n",
    "# \t\t# points_rev = [points_rev[i] for i in ConvexHull(points_rev).vertices]\n",
    "# \t\t# if color == 'orange':\n",
    "# \t\t\t# display(positions)\n",
    "# \t# sim_geo = gpd.GeoSeries(point).simplify(tolerance=0.001)\n",
    "# \t# geo_j = sim_geo.to_json()\n",
    "# \t# rrr = gpd.GeoSeries(point).simplify(tolerance=0.001).to_json()\n",
    "# \tfolium.GeoJson(data=pd.Series(point),  style_function=lambda feature: {\n",
    "#             'fillColor': color,\n",
    "#             'fillOpacity': 0.8,\n",
    "#         }).add_to(base_map)\n",
    "\t# geo_j = folium.GeoJson(data=geo_j,  style_function=lambda x: {'fillColor': 'orange'})\n",
    "\t# folium.Popup(r['BoroName']).add_to(geo_j)\n",
    "\t# geo_j.add_to(base_map)\n",
    "\n",
    "\t# plugins.PolyLineOffset(locations=point, color='blue', fill=True, fill_color=color, fill_opacity=0.5, smooth_factor=.1).add_to(base_map)\n",
    "\n",
    "# lats, longs = xy_to_latlon(df2009.xcoord.to_list(), df2009.ycoord.to_list(), xy_to_latlon)\n",
    "# for lat, lon, i in zip(lats, longs, df2009.arstmade):\n",
    "#     color = 'blue' if i == 'Y' else '#FF000030'\n",
    "#     size = '20' if i == 'Y' else '10'\n",
    "#     border = 'none' if i == 'Y' else 'none'\n",
    "#     folium.Marker(location=[str(lon), str(lat)], icon=folium.DivIcon(html=f\"<span style='font-size:{size}px;color:{color};border:{border}'>&#9670;</span>\"),popup=i).add_to(base_map)\n",
    "base_map\n",
    "#otra opción de crear mapa con folium:\n",
    "import folium\n",
    "def generateBaseMap(loc, zoom=11, tiles='OpenStreetMap', crs='ESPG3857'):\n",
    "    return folium.Map(location=loc,\n",
    "                   #control_scale=True, \n",
    "                   zoom_start=zoom,\n",
    "                   #tiles=tiles)\n",
    "    )\n",
    "base_map = generateBaseMap([40.7127837, -74.0059413])\n",
    "\n",
    "\n",
    "marker = list(range(len(df2009.xcoord)))\n",
    "counter = 0\n",
    "tooltip = \"Click Here For More Info\"\n",
    "icon = folium.features.CustomIcon('https://cdn-icons-png.flaticon.com/128/7500/7500224.png', icon_size=(40, 40))\n",
    "\n",
    "for x,y in zip(df2009.xcoord, df2009.ycoord):\n",
    "    lon, lat = xy_to_latlon(x,y)\n",
    "    marker[counter] = folium.Marker(icon=icon,\n",
    "    #     #location=[40.7127837, -74.0059413],\n",
    "    location=[lon, lat],\n",
    "    #     #popup=\"<stong>Allianz Arena</stong>\",\n",
    "    #     #tooltip=tooltip\n",
    "    )\n",
    "    marker[counter].add_to(base_map)\n",
    "    #print(f\"latitud {lat} y longitud {lon}\")\n",
    "    if counter>5 : \n",
    "        break\n",
    "    counter += 1\n",
    "base_map\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentarios de los gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Problemas de clasificación\n",
    "### Definición de Clasificadores Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificadores base\n",
    "gnb     = GaussianNB()\n",
    "knc     = KNeighborsClassifier()\n",
    "svc     = SVC(random_state=42, probability=True)\n",
    "gbc     = GradientBoostingClassifier(random_state=42)\n",
    "lr      = LogisticRegression(random_state=42, C=0.01)\n",
    "rfc     = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "\n",
    "# metaestimator stacking\n",
    "sc_1 = StackingClassifier(estimators=[(\"lr\", lr), (\"rfc\", rfc)], final_estimator=gbc, cv=5)\n",
    "# En este caso se eligieron clasificadores de naturaleza distinta en los estimadores. En el final estimator se selecciono un gradient boosting.\n",
    "\n",
    "# metaestimator voting\n",
    "vc_1 = VotingClassifier([(\"lr\", lr), (\"knc\", knc), (\"gbc\", gbc)], voting=\"hard\", n_jobs=-1)\n",
    "# Se eligio un número impar de modelos para que siempre exista mayoría. \n",
    "\n",
    "# metaestimator boosting\n",
    "bc_1 = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5), random_state=42, n_estimators=100, learning_rate=1)\n",
    "\n",
    "nn_arch = {\n",
    "    'input_layer'       : ('input_dense', 32, 'relu'),\n",
    "    'drop_1'            : ('dropout', .2),\n",
    "    'output_layer'      : ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "# Red neuronal\n",
    "kcc = KerasCustomClassifier(    \n",
    "                                nn_arch, \n",
    "                                loss='binary_crossentropy',\n",
    "                                optimizer='Adam',\n",
    "                                metrics='accuracy',\n",
    "                                net_name='twitter_keras_net',\n",
    "                                epochs=10\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del main pipeline dependiendo de la métrica a optimizar y del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solución para usar SMOTE en gridsearch con muchos pasos: https://stackoverflow.com/questions/65652054/not-able-to-feed-the-combined-smote-randomundersampler-pipeline-into-the-main\n",
    "\n",
    "\n",
    "def main_pipeline(estimator, type='accuracy', problema='arstmade'):\n",
    "    if problema == 'arstmade':\n",
    "        if type=='accuracy':\n",
    "            return Imb_Pipeline(steps=[\n",
    "                ('ce', FunctionTransformer(utils.criterio_experto, kw_args={'columns': var_eliminar_por_corr + var_eliminar_pf})),\n",
    "                ('csd', CreateSuitableDataframeTransformer()),\n",
    "                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "                ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "                ('num_imp', MeanMedianImputer(imputation_method='median')),\n",
    "                ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "                ('model', estimator)\n",
    "            ])\n",
    "        \n",
    "        return Imb_Pipeline(steps=[\n",
    "            ('csd', CreateSuitableDataframeTransformer()),\n",
    "            ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "            ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),\n",
    "            ('num_imp', MeanMedianImputer(imputation_method='median')),\n",
    "            ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "            ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "            ('model', estimator)\n",
    "        ])\n",
    "    return Imb_Pipeline(steps=[\n",
    "        ('csd', CreateSuitableDataframeTransformer()),\n",
    "        ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "        ('oe', OrdinalEncoderFixedTransformer(encoding_method='arbitrary')),\n",
    "        ('num_imp', MeanMedianImputer(imputation_method='median')),\n",
    "        ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "\n",
    "def gridsearch_train_and_save(pipe,x,y, params, scoring='accuracy', file_name='file.pickle'):\n",
    "    search = GridSearchCV(pipe, params, cv=5, scoring=scoring, n_jobs=4, error_score='raise', verbose=10)\n",
    "    search.fit(x, y)\n",
    "    utils.save_bytes_variable({'best_params': search.best_params_, 'best_score': search.best_score_, 'results': pd.DataFrame(search.cv_results_), 'params': params}, file_name)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se fijaron parámetros del preprocesamiento en función de un gridsearch que se corrió con un estimador modelo (Random Forest) que permitiera encontrar los parámetros óptimos de esta etapa para luego poder centrarnos solo en refinar los parámetros del modelo elegido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Procedimiento que termina en arresto `(Arstmade)`\n",
    "### Optimización del sub-pipeline de preprocessing del problema y_1 (accuracy y recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(set(var_eliminar_por_corr + var_eliminar_others + var_eliminar_pf))\n",
    "l2 = list(set(var_eliminar_por_corr + var_eliminar_pf))\n",
    "l3 = list(set(var_eliminar_por_corr + var_eliminar_others))\n",
    "params = {\n",
    "        'ce': [     \n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l1}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l2}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l3}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':[]}),        \n",
    "                ],\n",
    "        # 'smote': [SMOTE(sampling_strategy='auto',random_state=42), None],\n",
    "        'cat_imp__imputation_method': ['frequent', 'missing'],\n",
    "        'num_imp__imputation_method': ['median', 'mean'],\n",
    "        'oe__encoding_method': ['ordered', 'arbitrary']\n",
    "}\n",
    "\n",
    "pipe1 = main_pipeline(rfc)\n",
    "pipe2 = main_pipeline(rfc)\n",
    "\n",
    "search_acc = gridsearch_train_and_save(pipe1,x_1,y_1, params, scoring='accuracy', file_name='gridsearch_preprocessing_crimenes_accuracy.pickle')\n",
    "search_rec = gridsearch_train_and_save(pipe2,x_1,y_1, params, scoring='recall', file_name='gridsearch_preprocessing_crimenes_recall.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_acc.best_params_, search_acc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rec.best_params_, search_rec.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de modelos en accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 1: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking = main_pipeline(sc_1)\n",
    "\n",
    "params_stacking = {\n",
    "    'model__final_estimator__n_estimators': [50, 100, 500],\n",
    "    'model__final_estimator__max_depth': [5, 10, 50]\n",
    "}\n",
    "\n",
    "search_stacking = gridsearch_train_and_save(pipe_stacking,x_1,y_1, params_stacking, scoring='accuracy', file_name='gridsearch_stacking_crimenes_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 2: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_voting = main_pipeline(vc_1)\n",
    "\n",
    "params_voting = {\n",
    "                    'model__voting': ['hard','soft']\n",
    "                }\n",
    "\n",
    "search_voting = gridsearch_train_and_save(pipe_voting,x_1,y_1, params_voting, scoring='accuracy', file_name='gridsearch_voting_crimenes_accuracy.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 3: GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gboost = main_pipeline(gbc)\n",
    "\n",
    "params_gboost = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'model__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "search_gboost = gridsearch_train_and_save(\n",
    "    pipe_gboost, x_1, y_1, params_gboost, scoring='accuracy', file_name='gridsearch_gboost_crimenes_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 4: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_adaboost = main_pipeline(bc_1)\n",
    "\n",
    "params_adaboost = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "search_adaboost = gridsearch_train_and_save(\n",
    "    pipe_adaboost, x_1, y_1, params_adaboost, scoring='accuracy', file_name='gridsearch_adaboost_crimenes_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 5: Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nn = main_pipeline(kcc)\n",
    "\n",
    "nn_arch_1 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_2 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_3 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_4 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_5 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_6 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "params_nn = {\n",
    "    'model__epochs': [5, 10, 20],\n",
    "    'model__nn_arch': [nn_arch_1, nn_arch_2, nn_arch_3, nn_arch_4, nn_arch_5, nn_arch_6],\n",
    "}\n",
    "\n",
    "\n",
    "search_nn = gridsearch_train_and_save(pipe_nn, x_1,y_1, params_nn, scoring='accuracy', file_name='gridsearch_nn_crimenes_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de modelos en recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 1: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking_r = main_pipeline(sc_1, type='recall')\n",
    "\n",
    "params_stacking_r = {\n",
    "    'model__final_estimator__n_estimators': [50, 100, 500],\n",
    "    'model__final_estimator__max_depth': [5, 10, 50]\n",
    "}\n",
    "\n",
    "search_stacking_r = gridsearch_train_and_save(pipe_stacking_r,x_1,y_1, params_stacking_r, scoring='recall', file_name='gridsearch_stacking_crimenes_recall.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 2: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_voting_r = main_pipeline(vc_1, type='recall')\n",
    "\n",
    "params_voting_r = {\n",
    "                    'model__voting': ['hard','soft']\n",
    "                }\n",
    "\n",
    "search_voting_r = gridsearch_train_and_save(pipe_voting_r,x_1,y_1, params_voting_r, scoring='recall', file_name='gridsearch_voting_crimenes_recall.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 3: GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gboost_r = main_pipeline(gbc, type='recall')\n",
    "\n",
    "params_gboost_r = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'model__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "search_gboost_r = gridsearch_train_and_save(\n",
    "    pipe_gboost_r, x_1, y_1, params_gboost_r, scoring='recall', file_name='gridsearch_gboost_crimenes_recall.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 4: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_adaboost_r = main_pipeline(bc_1, type='recall')\n",
    "\n",
    "params_adaboost_r = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "search_adaboost_r = gridsearch_train_and_save(\n",
    "    pipe_adaboost_r, x_1, y_1, params_adaboost_r, scoring='recall', file_name='gridsearch_adaboost_crimenes_recall.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 5: Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nn_r = main_pipeline(kcc, type='recall')\n",
    "\n",
    "nn_arch_1 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_2 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_3 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_4 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_5 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_6 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "params_nn_r = {\n",
    "    'model__epochs': [5, 10, 20],\n",
    "    'model__nn_arch': [nn_arch_1, nn_arch_2, nn_arch_3, nn_arch_4, nn_arch_5, nn_arch_6],\n",
    "}\n",
    "\n",
    "\n",
    "search_nn_r = gridsearch_train_and_save(pipe_nn_r, x_1,y_1, params_nn_r, scoring='recall', file_name='gridsearch_nn_crimenes_recall.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Procedimiento con violencia `(Violence)`\n",
    "### Optimización del sub-pipeline de preprocessing del problema y_2 (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(set(var_eliminar_por_corr + var_eliminar_others + var_eliminar_pf))\n",
    "l2 = list(set(var_eliminar_por_corr + var_eliminar_pf))\n",
    "l3 = list(set(var_eliminar_por_corr + var_eliminar_others))\n",
    "params = {\n",
    "        'ce': [     \n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l1}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l2}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':l3}),\n",
    "                FunctionTransformer(utils.criterio_experto, kw_args={'columns':[]}),        \n",
    "                ],\n",
    "        'smote': [SMOTE(sampling_strategy='auto',random_state=42), None],\n",
    "        'cat_imp__imputation_method': ['frequent', 'missing'],\n",
    "        'num_imp__imputation_method': ['median', 'mean'],\n",
    "        'oe__encoding_method': ['ordered', 'arbitrary']\n",
    "}\n",
    "\n",
    "pipe1_v = main_pipeline(rfc, problema='violence')\n",
    "pipe2_v = main_pipeline(rfc, problema='violence')\n",
    "\n",
    "search_acc_v = gridsearch_train_and_save(pipe1_v, x_2, y_2, params, scoring='accuracy', file_name='gridsearch_preprocessing_crimenes_violence_accuracy.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de modelos en accuracy\n",
    "#### GridSearch de Modelo 1: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking_v = main_pipeline(sc_1, problema='violence')\n",
    "\n",
    "params_stacking_v = {\n",
    "    'model__final_estimator__n_estimators': [50, 100, 500],\n",
    "    'model__final_estimator__max_depth': [5, 10, 50]\n",
    "}\n",
    "\n",
    "search_stacking_v = gridsearch_train_and_save(\n",
    "    pipe_stacking_v,x_2, y_2, params_stacking_v, scoring='accuracy', file_name='gridsearch_stacking_crimenes_violence_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 2: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_voting_v = main_pipeline(vc_1, problema='violence')\n",
    "\n",
    "params_voting_v = {\n",
    "                    'model__voting': ['hard','soft']\n",
    "                }\n",
    "\n",
    "search_voting_v = gridsearch_train_and_save(\n",
    "    pipe_voting_v, x_2, y_2, params_voting_v, scoring='accuracy', file_name='gridsearch_voting_crimenes_violence_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 3: GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gboost_v = main_pipeline(gbc, problema='violence')\n",
    "\n",
    "params_gboost_v = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'model__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "search_gboost_v = gridsearch_train_and_save(pipe_gboost_v, x_2, y_2, params_gboost_v, scoring='accuracy', file_name='gridsearch_gboost_crimenes_violence_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 4: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_adaboost_v = main_pipeline(bc_1, problema='violence')\n",
    "\n",
    "params_adaboost_v = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "search_adaboost_v = gridsearch_train_and_save(pipe_adaboost_v, x_2, y_2, params_adaboost_v,\n",
    "                                            scoring='accuracy', file_name='gridsearch_adaboost_crimenes_violence_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch de Modelo 5: Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nn_v = main_pipeline(kcc, problema='violence')\n",
    "\n",
    "nn_arch_1 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_2 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .2),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_3 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_4 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .1),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_5 = {\n",
    "    'input_layer': ('input_dense', 32, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "nn_arch_6 = {\n",
    "    'input_layer': ('input_dense', 16, 'relu'),\n",
    "    'drop_1': ('dropout', .4),\n",
    "    'output_layer': ('dense', 1, 'sigmoid')\n",
    "}\n",
    "\n",
    "params_nn_v = {\n",
    "    'model__epochs': [5, 10, 20],\n",
    "    'model__nn_arch': [nn_arch_1, nn_arch_2, nn_arch_3, nn_arch_4, nn_arch_5, nn_arch_6],\n",
    "}\n",
    "\n",
    "\n",
    "search_nn_v = gridsearch_train_and_save(\n",
    "    pipe_nn_v, x_2, y_2, params_nn_v, scoring='accuracy', file_name='gridsearch_nn_crimenes_violence_accuracy.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__voting': 'soft'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.load_bytes_variable('gridsearch_voting_crimenes_accuracy.pickle')[\n",
    "    'best_params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación por ACCURACY\n",
      "0.963103448275862 - gridsearch_adaboost_crimenes_accuracy.pickle\n",
      "0.9673259176863181 - gridsearch_gboost_crimenes_accuracy.pickle\n",
      "0.9659920899765171 - gridsearch_nn_crimenes_accuracy.pickle\n",
      "0.9602123346928686 - gridsearch_stacking_crimenes_accuracy.pickle\n",
      "0.965769620566061 - gridsearch_voting_crimenes_accuracy.pickle\n"
     ]
    }
   ],
   "source": [
    "print('Comparación por ACCURACY')\n",
    "for file in glob.glob(\"*crimenes_accuracy*\"):\n",
    "    if 'preprocessing' not in file:\n",
    "        dct = utils.load_bytes_variable(file)\n",
    "        print(f'{dct[\"best_score\"]} - {file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación por RECALL\n",
      "0.507077922077922 - gridsearch_adaboost_crimenes_recall.pickle\n",
      "0.6085064935064934 - gridsearch_gboost_crimenes_recall.pickle\n",
      "0.5620537357379463 - gridsearch_nn_crimenes_recall.pickle\n",
      "0.5616883116883116 - gridsearch_stacking_crimenes_recall.pickle\n",
      "0.5361038961038961 - gridsearch_voting_crimenes_recall.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomas\\.conda\\envs\\geopandas\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator FunctionTransformer from version 1.1.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Comparación por RECALL')\n",
    "for file in glob.glob(\"*crimenes_recall*\"):\n",
    "    if 'preprocessing' not in file:\n",
    "        dct = utils.load_bytes_variable(file)\n",
    "        print(f'{dct[\"best_score\"]} - {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__learning_rate': 0.5, 'model__n_estimators': 50}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.load_bytes_variable('gridsearch_adaboost_crimenes_violence_accuracy.pickle')['best_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación en problema de clasificación de violence por ACCURACY\n",
      "0.7721710542578173 - gridsearch_adaboost_crimenes_violence_accuracy.pickle\n",
      "0.7777258682486714 - gridsearch_gboost_crimenes_violence_accuracy.pickle\n",
      "0.7668347546656779 - gridsearch_nn_crimenes_violence_accuracy.pickle\n",
      "0.7703920405388703 - gridsearch_stacking_crimenes_violence_accuracy.pickle\n",
      "0.7717251266839698 - gridsearch_voting_crimenes_violence_accuracy.pickle\n"
     ]
    }
   ],
   "source": [
    "print('Comparación en problema de clasificación de violence por ACCURACY')\n",
    "for file in glob.glob(\"*crimenes_violence_accuracy*\"):\n",
    "    if 'preprocessing' not in file:\n",
    "        dct = utils.load_bytes_variable(file)\n",
    "        print(f'{dct[\"best_score\"]} - {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Serialización de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamos de serializar los modelos con mejor desempeño entre los cuales está la red neuronal implementada en Keras, pero nos fue imposible guardarla de manera apropiada para que pudiera ser testeada posteriormente.\n",
    "\n",
    "Teniendo en cuenta esto y bajo el criterio de seleccionar los modelos con mejor desempeño en el grid search, se determina que serializaremos los siguientes modelos:\n",
    "\n",
    "- Gradient boosting con los siguientes parámetros:\n",
    "```py\n",
    "  {\n",
    "    'model__learning_rate': 0.01,\n",
    "    'model__max_depth': 2,\n",
    "    'model__n_estimators': 200\n",
    "  }\n",
    "```\n",
    "\n",
    "- Voting Classifier con los siguientes parámetros:\n",
    "```py\n",
    "  {\n",
    "    'model__voting': 'soft'\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_opt = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=200, random_state=42)\n",
    "best_pipe_1 = main_pipeline(gbc_opt)\n",
    "\n",
    "vc_opt = VotingClassifier([(\"lr\", lr), (\"knc\", knc), (\"gbc\", gbc)], voting=\"soft\", n_jobs=-1)\n",
    "best_pipe_2 = main_pipeline(vc_opt)\n",
    "\n",
    "best_pipe_1.fit(x_1, y_1)\n",
    "best_pipe_2.fit(x_1, y_1)\n",
    "\n",
    "utils.save_bytes_variable({'best_pipe_1': best_pipe_1, 'best_pipe_2': best_pipe_2, 'data_row_drop': drt}, 'crimenes_best_models.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMA VIOLENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamos de serializar los modelos con mejor desempeño entre los cuales está la red neuronal implementada en Keras, pero nos fue imposible guardarla de manera apropiada para que pudiera ser testeada posteriormente.\n",
    "\n",
    "Teniendo en cuenta esto y bajo el criterio de seleccionar los modelos con mejor desempeño en el grid search, se determina que serializaremos los siguientes modelos:\n",
    "\n",
    "- Gradient boosting con los siguientes parámetros:\n",
    "```py\n",
    "  {\n",
    "    'model__learning_rate': 0.1,\n",
    "    'model__max_depth': 2,\n",
    "    'model__n_estimators': 200\n",
    "  }\n",
    "```\n",
    "\n",
    "- Adaboost con los siguientes parámetros:\n",
    "```py\n",
    "  {\n",
    "    'model__learning_rate': 0.5,\n",
    "    'model__n_estimators': 50\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_opt_v = GradientBoostingClassifier(learning_rate=0.1, max_depth=2, n_estimators=200, random_state=42)\n",
    "best_pipe_1_v = main_pipeline(gbc_opt_v, problema='violence')\n",
    "\n",
    "bc_opt_v = AdaBoostClassifier(\n",
    "                                base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5),random_state=42,\n",
    "                                n_estimators=50,\n",
    "                                learning_rate=0.5\n",
    "                            )\n",
    "best_pipe_2_v = main_pipeline(bc_opt_v, problema='violence')\n",
    "\n",
    "best_pipe_1_v.fit(x_2, y_2)\n",
    "best_pipe_2_v.fit(x_2, y_2)\n",
    "\n",
    "utils.save_bytes_variable({'best_pipe_1': best_pipe_1_v, 'best_pipe_2': best_pipe_2_v, 'data_row_drop': drt}, 'crimenes_violence_best_models.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geopandas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cfc4558a56c74004f66639ccbe2b85f126442f2f38d8bdd913543c2f1bf5fd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
