{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Machine Learning Prueba 2 - Analizando los crímenes en la Ciudad de Nueva York***.\n",
    "### Nombre(s): Thomas Peet, Braulio Águila, Camilo Ramírez\n",
    "### Generación: G47\n",
    "### Profesores: Alfonso Tobar - Sebastián Ulloa\n",
    "### Fecha: 06-10-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Contexto*\n",
    "En esta ocasión trabajaremos con datos públicos del departamento de policía de New York.    \n",
    "El dataset es llamado stop_and_frisk_data y contiene información sobre interrogaciones    \n",
    "y detenciones realizadas por el departamento de policía de NY en la vía pública. El    \n",
    "diccionario de atributos se encuentra en el archivo 2009 SQF File Spec.xls.    \n",
    "Para todo nuestro estudio utilizaremos los datos correspondientes al año 2009 como    \n",
    "conjunto de entrenamiento y los datos del 2010 como conjunto de pruebas. Hay que hacer    \n",
    "notar que los datos que estamos utilizando son un muestreo del de la cantidad de registros    \n",
    "reales que contiene el dataset, esta decisión fue tomada debido a los largos tiempos de    \n",
    "entrenamiento y procesamiento que requiere el volumen de datos reales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Objetivos*\n",
    "Para alcanzar el objetivo general, su trabajo se puede desagregar en los siguientes puntos:  \n",
    "\n",
    "1. Debe analizar de forma exploratoria los atributos. Reporte la cantidad de datos    \n",
    "perdidos y presente su esquema de recodificación. Tenga presente que lo que    \n",
    "observe en el análisis exploratorio debe guiar su proceso de ingeniería de atributos,    \n",
    "por lo que se le recomienda que piense en aspectos de las variables involucradas     \n",
    "que puedan afectar el proceso mencionado.\n",
    "\n",
    "2. Reporte la probabilidad de que un individuo sea arrestado en uno de los cinco  \n",
    "barrios, condicional al género y a la raza. Concluya, ¿qué implicancias éticas tienen  \n",
    "algunas conclusiones de lo que observa?.\n",
    "\n",
    "3. Entregue un modelo predictivo que prediga efectivamente si un determinado  \n",
    "procedimiento concluirá en un arresto o no. Para ello, guíate por los siguientes  \n",
    "lineamientos:  \n",
    "    - Entrene por lo menos 3 modelos que sean capaces de predecir si se  \n",
    "producirá un arresto o no. Una vez que encuentre un modelo satisfactorio,  \n",
    "reporte al menos dos métricas de desempeño.  \n",
    "    - Refine aquellos atributos relevantes con alguna estrategia que crea  \n",
    "conveniente y reporte por lo menos 5 atributos relevantes para realizar la  \n",
    "predicción.\n",
    "\n",
    "4. Genere al menos cinco modelos predictivos que permitan determinar si el  \n",
    "procedimiento policial concluirá en alguna acción violenta.  \n",
    "○ Para ello, debe generar un nuevo atributo como vector objetivo que indique  \n",
    "cuándo hubo violencia o no. Éste debe ser creado a partir de atributos  \n",
    "existentes que indiquen el tipo de violencia.\n",
    "  \n",
    "5. Seleccione los 2 mejores modelos, serialicelos y envíalos a evaluación. Recuerde que  \n",
    "el modelo serializado debe ser posterior al fit, para poder ejecutar predict en los  \n",
    "nuevos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Tipo de problema a resolver:\n",
    "- De acuerdo con el enunciado y una revisión preliminar de los datos entregados, ambas problemáticas planteadas,  \n",
    "el hecho de que ocurra o no un arresto, y de que un procedimiento policial es o no violento, corresponden a   \n",
    "problemas de **clasificación**, ya que ambas variables objetivos son discretas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Tipo de métricas a implementar:\n",
    "- Las métricas que se utilizarán para la división de muestras corresponden a :\n",
    "- Tipo de preprocesamiento: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Modelos (5) con gridsearch e hiperparamétros tentativos/definitivos:\n",
    "- Modelo 1:\n",
    "- Modelo 2:\n",
    "- Modelo 3:\n",
    "- Modelo 4:\n",
    "- Modelo 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Comportamiento de variables objetivo (recodificados):\n",
    "- Variable objetivo 1: procedimiento policial en el que ocurre o no un arresto (\"arstmade\")\n",
    "- Variable objetivo 2: procedimiento policial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "#from feature engine drop features ----> sacar features para el pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from pathlib import Path\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "#import shapely\n",
    "#import folium \n",
    "import pyproj\n",
    "import helpers as hp\n",
    "import preproc_nyc_sqf_V2 as preproc\n",
    "import contextily as cx\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import CreateSuitableDataframeTransformer\n",
    "from utils import OrdinalEncoderFixedTransformer\n",
    "from utils import DropRowsTransformer\n",
    "from utils import CriterioExperto\n",
    "import utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Dataset interrogaciones/detenciones Policía de Nueva York - 2009***:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de los datasets para los 2 problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se eliminan los datos sin xcoord o ycoord\n",
    "* Se eliminan los registros de detenidos menores de 18 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4499, 111), (4499,), (4499, 103), (4499,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2009 = pd.read_csv('2009_1perc.csv', index_col=0)\n",
    "\n",
    "drt = DropRowsTransformer()\n",
    "df2009_nonull = drt.transform(df2009)\n",
    "\n",
    "x_1, y_1, x_2, y_2 = utils.split_features_target(df2009_nonull)\n",
    "# El mismo dataframe aplicando los criterios de descartar datos sin xcoord/ycoord y registros de menores de 18 años\n",
    "x_1.shape, y_1.shape, x_2.shape, y_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de columnas eliminadas en base a \"Criterio Expertos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerar variables relevantes para modelos de procedimiento que concluye en arresto y/o procedimiento que concluye en situación violenta:\n",
    "# redefinir variable \"others\" (esta mál escrita con respecto a la planilla)\n",
    "\n",
    "var_eliminar_pf = [\"pf_baton\", \"pf_hcuff\", \"pf_pepsp\", \"pf_other\", \"pf_ptwep\", \"pf_drwep\", \"pf_wall\", \"pf_hands\", \"pf_grnd\"] # 9 variables eliminadas\n",
    "\n",
    "var_eliminar_others = [\"ac_rept\", \"ac_inves\", \"ac_proxm\", \"cs_casng\", \"cs_lkout\", \"explnstp\",\"sumissue\", \"offunif\", \"officrid\", \"frisked\", \"searched\", \"cs_cloth\", \"offverb\", \"rf_furt\", \"sb_other\", \"ac_other\", \"rf_bulg\", \"cs_furtv\", \"recstat\", \"cs_bulge\", \"cs_other\", \"trhsloc\", \"build\", \"beat\", \"post\",\"rf_attir\", \"cs_objcs\",\"eyecolor\", \"haircolr\", \"sector\" ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición de clasificadores base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se utiliza función entregada para limpieza preliminar de la data, se obtiene un df nuevo procesado y dos listas:\n",
    "# proc_df #este nuevo dataframe, estas constituido de 69 variables categóricas y 2 númericas sintéticas (\"meters\", \"month\")\n",
    "\n",
    "gnb = GaussianNB()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = SVC(random_state=42, probability=True)\n",
    "# funciona particularmente bien stackeando xgboost\n",
    "lr = LogisticRegression(random_state=42)\n",
    "rfc = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Definición del main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params = {\n",
    "#     # 'preprocessing__num_imp__imputation_method': ['mean', 'median'],\n",
    "#     'model__n_estimators': [10, 50, 100, 500],\n",
    "#     'model__max_depth': [5, 10, 50, 100, 300]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'model__learning_rate': [0.01, 0.02, 0.04],\n",
    "    'model__subsample': [0.9, 0.5, 0.2, 0.1],\n",
    "    'model__n_estimators': [100, 500, 1000],\n",
    "    'model__max_depth': [4, 7, 10]\n",
    "}\n",
    "# Solución para usar SMOTE en gridsearch con muchos pasos: https://stackoverflow.com/questions/65652054/not-able-to-feed-the-combined-smote-randomundersampler-pipeline-into-the-main\n",
    "\n",
    "def main_pipeline(estimator):\n",
    "    pipe = Imb_Pipeline(steps=[\n",
    "        ('ce', CriterioExperto(columns = var_eliminar_others + var_eliminar_pf)),\n",
    "        ('csd', CreateSuitableDataframeTransformer()),\n",
    "        ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n",
    "        ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "        ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered')),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas) para encodear.\n",
    "        ('smote', SMOTE(sampling_strategy='auto',random_state=42)),\n",
    "        ('sc', SklearnTransformerWrapper(StandardScaler())),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def gridsearch_train_and_save(pipe, params, scoring='accuracy', file_name='file.pickle'):\n",
    "    search = GridSearchCV(pipe, params, cv=5, scoring=scoring, n_jobs=-1, error_score='raise', verbose=10)\n",
    "    search.fit(x_1, y_1)\n",
    "\n",
    "    utils.save_bytes_variable({'best_params': search.best_params_, 'best_score': search.best_score_, 'pipe': pd.DataFrame(search.cv_results_), 'params': params}, file_name)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimización del sub-pipeline de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'smote': [SMOTE(sampling_strategy='auto',random_state=42), None],\n",
    "        'cat_imp__imputation_method': ['frequent', 'missing'],\n",
    "        'num_imp__imputation_method': ['median', 'mean'],\n",
    "        'oe__encoding_method': ['ordered', 'arbitrary']\n",
    "}\n",
    "\n",
    "pipe = main_pipeline(rfc)\n",
    "\n",
    "search_acc = gridsearch_train_and_save(pipe, params, scoring='accuracy', file_name='gridsearch_preprocessing_crimenes_accuracy.pickle')\n",
    "\n",
    "search_rec = gridsearch_train_and_save(pipe, params, scoring='recall', file_name='gridsearch_preprocessing_crimenes_recall.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_acc.best_params_, search_acc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rec.best_params_, search_rec.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Exploración variable objetivo 1 (\"arstmade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1.value_counts() #oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de el vector objetivo:\n",
    "sns.histplot(y_1, x=\"arstmade\"); # Categoría uno muchos menos casos que categoría 0.\n",
    "plt.title(\"Vector objetivo : arstmade\")\n",
    "plt.xlabel(\"clases\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>#### Exploración variable objetivo 2 (\"violencia física en arresto\") --- Necesita ser recodificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recodificando variable objetivo 2:\n",
    "\n",
    "var_pf = proc_df.columns[np.where([i[0:2]=='pf' for i in proc_df.columns.tolist()])]. tolist()\n",
    "u = proc_df[var_pf]\n",
    "u[\"var_pf\"] = [int(np.isin([\"Y\"], u.iloc[i].values.tolist())[0]) for i in range(0,len(u))]\n",
    "\n",
    "proc_df.insert(proc_df.shape[1],\"var_pf\", u[\"var_pf\"])\n",
    "proc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp.cat_num_rate_analysis(proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unbounded slice",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomas\\Desktop\\Tareas Data Science - Desafío Latam\\machine_learning\\prueba\\crimenes\\crimenesBA2.ipynb Celda 32\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m u \u001b[39m=\u001b[39m df2009[var_pf]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# df_probs['violence'] = \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_probs\u001b[39m.\u001b[39;49minsert(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mviolence\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39mint\u001b[39;49m(np\u001b[39m.\u001b[39;49misin([\u001b[39m\"\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m\"\u001b[39;49m], u\u001b[39m.\u001b[39;49miloc[i]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mtolist())[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(u))])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m probs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df_probs\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrace\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msex\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39marstmade\u001b[39m.\u001b[39mvalue_counts\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m (normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m counts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df_probs\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrace\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msex\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39marstmade\u001b[39m.\u001b[39mvalue_counts\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomas/Desktop/Tareas%20Data%20Science%20-%20Desaf%C3%ADo%20Latam/machine_learning/prueba/crimenes/crimenesBA2.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m (normalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:4445\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mloc must be int\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4444\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m-> 4445\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minsert(loc, column, value)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:1241\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1239\u001b[0m     value \u001b[39m=\u001b[39m ensure_block_shape(value, ndim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)\n\u001b[1;32m-> 1241\u001b[0m bp \u001b[39m=\u001b[39m BlockPlacement(\u001b[39mslice\u001b[39;49m(loc, loc \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[0;32m   1242\u001b[0m block \u001b[39m=\u001b[39m new_block_2d(values\u001b[39m=\u001b[39mvalue, placement\u001b[39m=\u001b[39mbp)\n\u001b[0;32m   1244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m   1245\u001b[0m     \u001b[39m# Fastpath\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\internals.pyx:56\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockPlacement.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\internals.pyx:318\u001b[0m, in \u001b[0;36mpandas._libs.internals.slice_canonize\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unbounded slice"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_probs = df2009\n",
    "var_pf = df2009.columns[np.where([i[0:2]=='pf' for i in df2009.columns.tolist()])]. tolist()\n",
    "u = df2009[var_pf]\n",
    "# df_probs['violence'] = \n",
    "df_probs.insert(-1, 'violence', pd.Series([int(np.isin([\"Y\"], u.iloc[i].values.tolist())[0]) for i in range(0, len(u))]))\n",
    "\n",
    "probs = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=True))\n",
    "counts = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).arstmade.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "probs_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=True))\n",
    "counts_v = pd.DataFrame(df_probs.groupby(['city', 'race', 'sex']).violence.value_counts\n",
    "(normalize=False))\n",
    "\n",
    "# Sex: F, M, Z\n",
    "# Race: A, B, I, P, Q, U, W, X, Z\n",
    "# City: BRONX, QUEENS, STATEN ISLAND, MANHATTAN, BROOKLYN\n",
    "\n",
    "print(counts.query(\"race == 'B' & sex == 'M' \"))\n",
    "print(counts.query(\"race == 'W' & sex == 'M' \"))\n",
    "print('--------------')\n",
    "print(counts_v.query(\"race == 'B' & sex == 'M' \"))\n",
    "print(counts_v.query(\"race == 'W' & sex == 'M' \"))\n",
    "\n",
    "df_probs.violence\n",
    "## Este caso no se observa lo que se esperaba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2009.shape, df_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar variables (Considerar drop_features)\n",
    "#dropna(axis = 1, inplace= True)\n",
    "#df_na = df1.dropna(how=\"all\", axis=1)  \n",
    "#dfnan = df1.dropna(subset=[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formato para definir variables elegidas posterior al filtro\n",
    "\n",
    "# # ***Listado de variables***:\n",
    "# * `age`: Edad del individuo.\n",
    "# * `workclass`: Naturaleza de la organización que emplea al individuo.\n",
    "# * `education`: Nivel educacional del individuo:\n",
    "#     | Variable          | Explicación                       |\n",
    "#     | ----------------- | ------------------                |\n",
    "#     | __Bachelors__     | (Licenciado)                      |\n",
    "#     | __Some-college__  | (Superior incompleta)             |\n",
    "#     | __11th__          | (3ro medio)                       |\n",
    "#     | __HS-grad__       | (Secundaria completa)             |\n",
    "#     | __Prof-school__   | (Escuela profesional)             |\n",
    "#     | __ssoc-acdm__     | (Técnico superior administrativo) |\n",
    "#     | __Assoc-voc__     | (Técnico superior vocacional)     |\n",
    "#     | __9th__           | (1ro medio)                       |\n",
    "#     | __7th-8th__       | (7mo-8vo)                         |\n",
    "#     | __12th__          | (4to medio)                       |\n",
    "#     | __Masters__       | (Maestría de postgrado)           |\n",
    "#     | __1st-4th__       | (1ro-4to básico)                  |\n",
    "#     | __10th__          | (2do medio)                       |\n",
    "#     | __Doctorate__     | (Doctorado)                       |\n",
    "#     | __5th-6th__       | (5to-6to)                         |\n",
    "#     | __Preschool__     | (Preescolar).                     |\n",
    "     \n",
    "#     <br>  \n",
    "    \n",
    "# * `capital-gains`: Ingresos generados por inversiones fuera del trabajo asalariado\n",
    "# * `capital-losses`: Pérdidas generadas por inversiones fuera del trabajo asalariado.\n",
    "# * `fnlwgt`: Ponderador muestral.\n",
    "# * `marital-status`: Estado civil del individuo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa\n",
    "gdf = gpd.GeoDataFrame(proc_df, geometry=gpd.points_from_xy(proc_df.xcoord, proc_df.ycoord))\n",
    "df = gpd.read_file(gpd.datasets.get_path(\"nybb\"))\n",
    "df_wm = df.to_crs(epsg=3857)\n",
    "ax = df.plot(figsize=(15,10), alpha=0.5, edgecolor = \"k\")\n",
    "cx.add_basemap(ax,crs=df.crs, zoom =11, source=cx.providers.Stamen.TonerLite)\n",
    "#cx.add_basemap(ax, source=cx.providers.Stamen.TonerLabels)\n",
    "gdf.plot(ax=ax, color= \"red\", edgecolor = \"black\", markersize = 4)\n",
    "\n",
    "\n",
    "#Incorporar al código de arriba para poder plotear variable objetivo:\n",
    "# fig,ax = plt.subplots(1, 1) #plt.figure(figsize=(10,8))\n",
    "# fig.set_size_inches(25, 25)\n",
    "# df2009map[df2009map.arstmade == \"N\"].plot(color=\"red\", ax=ax, markersize = 1)\n",
    "# df2009map[df2009map.arstmade == \"Y\"].plot(ax=ax,  markersize = 12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función transformar x,y to lon, lat:\n",
    "def xy_to_latlon(x,y):\n",
    "    source_crs = 'epsg:2263' # Coordinate system of the file\n",
    "    target_crs = 'epsg:4326' # Global lat-lon coordinate system\n",
    "    polar_to_latlon = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    lat, lon = polar_to_latlon.transform(x,y)\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otra opción de crear mapa con folium:\n",
    "import folium\n",
    "def generateBaseMap(loc, zoom=11, tiles='OpenStreetMap', crs='ESPG3857'):\n",
    "    return folium.Map(location=loc,\n",
    "                   #control_scale=True, \n",
    "                   zoom_start=zoom,\n",
    "                   #tiles=tiles)\n",
    "    )\n",
    "base_map = generateBaseMap([40.7127837, -74.0059413])\n",
    "\n",
    "\n",
    "marker = list(range(len(df2009.xcoord)))\n",
    "counter = 0\n",
    "tooltip = \"Click Here For More Info\"\n",
    "icon = folium.features.CustomIcon('https://cdn-icons-png.flaticon.com/128/7500/7500224.png', icon_size=(40, 40))\n",
    "\n",
    "for x,y in zip(df2009.xcoord, df2009.ycoord):\n",
    "    lon, lat = xy_to_latlon(x,y)\n",
    "    marker[counter] = folium.Marker(icon=icon,\n",
    "    #     #location=[40.7127837, -74.0059413],\n",
    "    location=[lon, lat],\n",
    "    #     #popup=\"<stong>Allianz Arena</stong>\",\n",
    "    #     #tooltip=tooltip\n",
    "    )\n",
    "    marker[counter].add_to(base_map)\n",
    "    #print(f\"latitud {lat} y longitud {lon}\")\n",
    "    if counter>5 : \n",
    "        break\n",
    "    counter += 1\n",
    "base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteo de nulos\n",
    "#proc_df.info(verbose=True, show_counts=True,memory_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descripción de variables númericas:\n",
    "proc_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descripción de variables categóricas: \n",
    "proc_df.describe(include=\"O\").T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar df2010\n",
    "# df2 = pd.read_csv('2010_1perc.csv', index_col='Unnamed: 0')\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd = CreateSuitableDataframeTransformer()\n",
    "x_1 = df2009.drop(columns=['arstmade']) \n",
    "preprocess = Pipeline(steps=[ \n",
    "    ('ce', CriterioExperto(columns = var_eliminar_others + var_eliminar_pf)), \n",
    "    ('csd', csd), \n",
    "    ('cat_imp', CategoricalImputer(imputation_method='frequent')), \n",
    "    ('num_imp', MeanMedianImputer(imputation_method='mean')), \n",
    "    ('oe', OrdinalEncoderFixedTransformer(encoding_method='ordered', csd=csd)),# aqui estoy pasando info desde el proceso csd (que son las columnas que seleccionamos como categoricas)\n",
    "    ('sc', SklearnTransformerWrapper(StandardScaler())) ,\n",
    "]) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geopandas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cfc4558a56c74004f66639ccbe2b85f126442f2f38d8bdd913543c2f1bf5fd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
