{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glb = glob.glob(os.getcwd()+\"/dump/*.csv\")\n",
    "li = []\n",
    "for filename in glb:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True).drop(columns='Unnamed: 0')\n",
    "\n",
    "df.columns = ['artist','genre','song','lyrics']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.artist.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=None, analyzer='word')\n",
    "X = cv.fit_transform(df.lyrics)\n",
    "X.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names_out().shape\n",
    "\n",
    "dff = pd.DataFrame(zip(X.toarray().sum(axis=0),cv.get_feature_names_out() )).sort_values(by=[0],ascending=False).loc[:5000]\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(dff.loc[:20].to_html(escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = Pipeline (steps= [\n",
    "    ('cv', CountVectorizer(stop_words='english', max_df=1, max_features=5000)),\n",
    "    ('lda', LatentDirichletAllocation(learning_method='online',n_components=10, n_jobs=-1,random_state=123))\n",
    "])\n",
    "\n",
    "# pipe.fit(df.lyrics)\n",
    "# words = pipe.named_steps.cv.get_feature_names_out()\n",
    "# topics = pipe.named_steps.lda.components_ / pipe.named_steps.lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "# topicos = pd.DataFrame(topics, columns=words).T\n",
    "# def call_top_topics(topics_df, topic_n, top_n):\n",
    "#     return topics_df[topic_n].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "# call_top_topics(topicos, 0, 5000)  # POP Romantica ... Britney Spears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(stop_words='english', max_df=1, max_features=5000)\n",
    "# transformed = cv.fit_transform(df.lyrics)\n",
    "\n",
    "# n_components = 10\n",
    "# learning_decay = .5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lda__n_components': [5, 10, 15],\n",
    "    'lda__learning_decay': [.7, .5] \n",
    "    }\n",
    "\n",
    "search = GridSearchCV(pipe, params, cv = 5, scoring = 'accuracy', n_jobs=-1, return_train_score=True, refit=True)\n",
    "search.fit(df.lyrics)\n",
    "search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediante .components_ podemos extraer una matriz que entrega las distribución de palabras por cada tópico.\n",
    "for topic_id, topic_name in enumerate(fit_best_lda.components_):\n",
    "    # para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud\n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([counter.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "X, _ = make_multilabel_classification(random_state=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9ccd8639d7ac6d8ae46f08631d02de0d1c9f4a08850208985333be71082afd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
