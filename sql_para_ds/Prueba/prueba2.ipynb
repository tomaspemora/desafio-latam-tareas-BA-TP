{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la Base de Datos\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_DBHOST')\n",
    "dbname = os.getenv('POSTGRES_DBNAME')\n",
    "# print(f'user_{user}_password_{password}_host_{host}_dbname_{dbname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(user=user, host=host, port=5432, password=password)\n",
    "conn.set_session(autocommit=True)\n",
    "\n",
    "# Obtención de Cursor\n",
    "cursor          = conn.cursor();\n",
    "name_Database   = \"prueba\";\n",
    "\n",
    "# Eliminación de base de datos en caso de existir\n",
    "cursor.execute(\"DROP DATABASE IF EXISTS prueba;\")\n",
    "\n",
    "# Creación de sentencia para la base de datos\n",
    "sqlCreateDatabase = \"create database \"+ name_Database +\";\"\n",
    "\n",
    "# Creacion de la base de datos en PostgreSQL\n",
    "cursor.execute(sqlCreateDatabase);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención y tratamiento de las columnas del CSV\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_cupid.csv')\n",
    "df.rename(columns = {'hispanic / latin':'hispanic_latin'}, inplace = True)\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "cols = df.columns.to_list()\n",
    "for index, col in enumerate(cols):\n",
    "    cols[index] +=' numeric'\n",
    "    \n",
    "cols = ', '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=dbname, user=user, host=host, port=5432, password=password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Eliminando las tablas train_cupid y test_cupid si existen.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS train_cupid;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS test_cupid;\")\n",
    "\n",
    "# Obteniendo un Cursor para las tablas\n",
    "name_table_train = f\"train_cupid ({cols})\"\n",
    "name_table_test = f\"test_cupid ({cols})\"\n",
    "\n",
    "# Creación de las sentecias para las tablas\n",
    "sqlTable_train = \"create table \"+name_table_train+';'\n",
    "sqlTable_test = \"create table \"+name_table_test+';'\n",
    "\n",
    "# Creando las tablas en PostgreSQL\n",
    "cursor.execute(sqlTable_train)\n",
    "cursor.execute(sqlTable_test)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los datos del CSV a las Tablas creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_cupid.csv', 'r') as f:    \n",
    "    next(f) # Saltar la fila de los encabezados del CSV.\n",
    "    cursor.copy_from(f, 'train_cupid', sep=',')\n",
    "\n",
    "with open('test_cupid.csv', 'r') as f:    \n",
    "    next(f) # Saltar la fila de los encabezados del CSV.\n",
    "    cursor.copy_from(f, 'test_cupid', sep=',')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadiendo columna indice a las tablas train_cupid y test_cupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE train_cupid ADD indice SERIAL PRIMARY KEY;')\n",
    "cursor.execute('ALTER TABLE test_cupid ADD indice SERIAL PRIMARY KEY;')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos de train_cupid con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_25248\\4121648969.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql('SELECT * FROM train_cupid', conn, index_col='indice')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>virgo</th>\n",
       "      <th>taurus</th>\n",
       "      <th>scorpio</th>\n",
       "      <th>pisces</th>\n",
       "      <th>libra</th>\n",
       "      <th>leo</th>\n",
       "      <th>gemini</th>\n",
       "      <th>aries</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_straight</th>\n",
       "      <th>sex_m</th>\n",
       "      <th>smokes_sometimes</th>\n",
       "      <th>smokes_trying_to_quit</th>\n",
       "      <th>smokes_when_drinking</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>body_type_overweight</th>\n",
       "      <th>body_type_regular</th>\n",
       "      <th>education_high_school</th>\n",
       "      <th>education_undergrad_university</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20077</th>\n",
       "      <td>33.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20078</th>\n",
       "      <td>22.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20079</th>\n",
       "      <td>28.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20080</th>\n",
       "      <td>31.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20081</th>\n",
       "      <td>27.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20081 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  height  virgo  taurus  scorpio  pisces  libra  leo  gemini  \\\n",
       "indice                                                                     \n",
       "1       35.0    70.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "2       38.0    68.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "3       23.0    71.0    0.0     0.0      0.0     1.0    0.0  0.0     0.0   \n",
       "4       29.0    66.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "5       29.0    67.0    0.0     1.0      0.0     0.0    0.0  0.0     0.0   \n",
       "...      ...     ...    ...     ...      ...     ...    ...  ...     ...   \n",
       "20077   33.0    63.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "20078   22.0    65.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "20079   28.0    64.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "20080   31.0    62.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "20081   27.0    73.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "\n",
       "        aries  ...  orientation_straight  sex_m  smokes_sometimes  \\\n",
       "indice         ...                                                  \n",
       "1         0.0  ...                   1.0    1.0               0.0   \n",
       "2         0.0  ...                   1.0    1.0               0.0   \n",
       "3         0.0  ...                   1.0    1.0               0.0   \n",
       "4         0.0  ...                   1.0    1.0               0.0   \n",
       "5         0.0  ...                   1.0    1.0               0.0   \n",
       "...       ...  ...                   ...    ...               ...   \n",
       "20077     0.0  ...                   1.0    0.0               0.0   \n",
       "20078     0.0  ...                   1.0    0.0               0.0   \n",
       "20079     0.0  ...                   1.0    0.0               0.0   \n",
       "20080     0.0  ...                   1.0    0.0               0.0   \n",
       "20081     0.0  ...                   1.0    1.0               0.0   \n",
       "\n",
       "        smokes_trying_to_quit  smokes_when_drinking  smokes_yes  \\\n",
       "indice                                                            \n",
       "1                         0.0                   0.0         0.0   \n",
       "2                         0.0                   0.0         0.0   \n",
       "3                         0.0                   0.0         0.0   \n",
       "4                         0.0                   0.0         0.0   \n",
       "5                         0.0                   0.0         0.0   \n",
       "...                       ...                   ...         ...   \n",
       "20077                     0.0                   0.0         0.0   \n",
       "20078                     0.0                   0.0         0.0   \n",
       "20079                     0.0                   0.0         0.0   \n",
       "20080                     0.0                   0.0         0.0   \n",
       "20081                     1.0                   0.0         0.0   \n",
       "\n",
       "        body_type_overweight  body_type_regular  education_high_school  \\\n",
       "indice                                                                   \n",
       "1                        0.0                1.0                    0.0   \n",
       "2                        0.0                1.0                    0.0   \n",
       "3                        0.0                1.0                    0.0   \n",
       "4                        0.0                0.0                    0.0   \n",
       "5                        0.0                1.0                    0.0   \n",
       "...                      ...                ...                    ...   \n",
       "20077                    0.0                0.0                    0.0   \n",
       "20078                    0.0                1.0                    0.0   \n",
       "20079                    0.0                0.0                    0.0   \n",
       "20080                    0.0                0.0                    0.0   \n",
       "20081                    0.0                0.0                    0.0   \n",
       "\n",
       "        education_undergrad_university  \n",
       "indice                                  \n",
       "1                                  0.0  \n",
       "2                                  0.0  \n",
       "3                                  1.0  \n",
       "4                                  1.0  \n",
       "5                                  1.0  \n",
       "...                                ...  \n",
       "20077                              0.0  \n",
       "20078                              1.0  \n",
       "20079                              0.0  \n",
       "20080                              0.0  \n",
       "20081                              1.0  \n",
       "\n",
       "[20081 rows x 98 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql('SELECT * FROM train_cupid', conn, index_col='indice')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20081 entries, 1 to 20081\n",
      "Data columns (total 98 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   age                             20081 non-null  float64\n",
      " 1   height                          20081 non-null  float64\n",
      " 2   virgo                           20081 non-null  float64\n",
      " 3   taurus                          20081 non-null  float64\n",
      " 4   scorpio                         20081 non-null  float64\n",
      " 5   pisces                          20081 non-null  float64\n",
      " 6   libra                           20081 non-null  float64\n",
      " 7   leo                             20081 non-null  float64\n",
      " 8   gemini                          20081 non-null  float64\n",
      " 9   aries                           20081 non-null  float64\n",
      " 10  aquarius                        20081 non-null  float64\n",
      " 11  cancer                          20081 non-null  float64\n",
      " 12  sagittarius                     20081 non-null  float64\n",
      " 13  asian                           20081 non-null  float64\n",
      " 14  hispanic_latin                  20081 non-null  float64\n",
      " 15  black                           20081 non-null  float64\n",
      " 16  indian                          20081 non-null  float64\n",
      " 17  pacific_islander                20081 non-null  float64\n",
      " 18  native_american                 20081 non-null  float64\n",
      " 19  middle_eastern                  20081 non-null  float64\n",
      " 20  colorado                        20081 non-null  float64\n",
      " 21  new_york                        20081 non-null  float64\n",
      " 22  oregon                          20081 non-null  float64\n",
      " 23  arizona                         20081 non-null  float64\n",
      " 24  hawaii                          20081 non-null  float64\n",
      " 25  montana                         20081 non-null  float64\n",
      " 26  wisconsin                       20081 non-null  float64\n",
      " 27  virginia                        20081 non-null  float64\n",
      " 28  spain                           20081 non-null  float64\n",
      " 29  nevada                          20081 non-null  float64\n",
      " 30  illinois                        20081 non-null  float64\n",
      " 31  vietnam                         20081 non-null  float64\n",
      " 32  ireland                         20081 non-null  float64\n",
      " 33  louisiana                       20081 non-null  float64\n",
      " 34  michigan                        20081 non-null  float64\n",
      " 35  texas                           20081 non-null  float64\n",
      " 36  united_kingdom                  20081 non-null  float64\n",
      " 37  massachusetts                   20081 non-null  float64\n",
      " 38  north_carolina                  20081 non-null  float64\n",
      " 39  idaho                           20081 non-null  float64\n",
      " 40  mississippi                     20081 non-null  float64\n",
      " 41  new_jersey                      20081 non-null  float64\n",
      " 42  florida                         20081 non-null  float64\n",
      " 43  minnesota                       20081 non-null  float64\n",
      " 44  georgia                         20081 non-null  float64\n",
      " 45  utah                            20081 non-null  float64\n",
      " 46  washington                      20081 non-null  float64\n",
      " 47  west_virginia                   20081 non-null  float64\n",
      " 48  connecticut                     20081 non-null  float64\n",
      " 49  tennessee                       20081 non-null  float64\n",
      " 50  rhode_island                    20081 non-null  float64\n",
      " 51  district_of_columbia            20081 non-null  float64\n",
      " 52  canada                          20081 non-null  float64\n",
      " 53  missouri                        20081 non-null  float64\n",
      " 54  germany                         20081 non-null  float64\n",
      " 55  pennsylvania                    20081 non-null  float64\n",
      " 56  netherlands                     20081 non-null  float64\n",
      " 57  switzerland                     20081 non-null  float64\n",
      " 58  mexico                          20081 non-null  float64\n",
      " 59  ohio                            20081 non-null  float64\n",
      " 60  agnosticism                     20081 non-null  float64\n",
      " 61  atheism                         20081 non-null  float64\n",
      " 62  catholicism                     20081 non-null  float64\n",
      " 63  buddhism                        20081 non-null  float64\n",
      " 64  judaism                         20081 non-null  float64\n",
      " 65  hinduism                        20081 non-null  float64\n",
      " 66  islam                           20081 non-null  float64\n",
      " 67  pro_dogs                        20081 non-null  float64\n",
      " 68  pro_cats                        20081 non-null  float64\n",
      " 69  spanish                         20081 non-null  float64\n",
      " 70  chinese                         20081 non-null  float64\n",
      " 71  french                          20081 non-null  float64\n",
      " 72  german                          20081 non-null  float64\n",
      " 73  single                          20081 non-null  float64\n",
      " 74  seeing_someone                  20081 non-null  float64\n",
      " 75  available                       20081 non-null  float64\n",
      " 76  employed                        20081 non-null  float64\n",
      " 77  income_between_25_50            20081 non-null  float64\n",
      " 78  income_between_50_75            20081 non-null  float64\n",
      " 79  income_over_75                  20081 non-null  float64\n",
      " 80  drugs_often                     20081 non-null  float64\n",
      " 81  drugs_sometimes                 20081 non-null  float64\n",
      " 82  drinks_not_at_all               20081 non-null  float64\n",
      " 83  drinks_often                    20081 non-null  float64\n",
      " 84  drinks_rarely                   20081 non-null  float64\n",
      " 85  drinks_socially                 20081 non-null  float64\n",
      " 86  drinks_very_often               20081 non-null  float64\n",
      " 87  orientation_gay                 20081 non-null  float64\n",
      " 88  orientation_straight            20081 non-null  float64\n",
      " 89  sex_m                           20081 non-null  float64\n",
      " 90  smokes_sometimes                20081 non-null  float64\n",
      " 91  smokes_trying_to_quit           20081 non-null  float64\n",
      " 92  smokes_when_drinking            20081 non-null  float64\n",
      " 93  smokes_yes                      20081 non-null  float64\n",
      " 94  body_type_overweight            20081 non-null  float64\n",
      " 95  body_type_regular               20081 non-null  float64\n",
      " 96  education_high_school           20081 non-null  float64\n",
      " 97  education_undergrad_university  20081 non-null  float64\n",
      "dtypes: float64(98)\n",
      "memory usage: 15.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import  MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "import helpers\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector objetivo: **single**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['single', 'seeing_someone', 'available']\n",
    "data = {}\n",
    "for target in targets:\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "    data[target] = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'X_train': X_train,\n",
    "        'X_valid': X_valid,\n",
    "        'y_train': y_train,\n",
    "        'y_valid': y_valid,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    'm1' : GradientBoostingClassifier(random_state=42),\n",
    "    'm2' : AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5), random_state=42, n_estimators=100, learning_rate=1),\n",
    "    'm3' : RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
    "    'm4' : SVC(random_state=42, probability=True),\n",
    "    'm5' : DecisionTreeClassifier(random_state=42),\n",
    "    'm6' : LogisticRegression(random_state=42, C=0.01),\n",
    "    'm7' : BernoulliNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Pipeline(steps=[\n",
    "    ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "    ('ord', OrdinalEncoder(encoding_method='ordered', variables='age', ignore_format=True)),\n",
    "    ('sc', SklearnTransformerWrapper(StandardScaler(), variables=['age', 'height']))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit y serialización de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single\n",
      "GradientBoostingClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__GradientBoostingClassifier__1211-21.pkl\n",
      "AdaBoostClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__AdaBoostClassifier__1211-21.pkl\n",
      "RandomForestClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__RandomForestClassifier__1211-21.pkl\n",
      "SVC\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__SVC__1211-21.pkl\n",
      "DecisionTreeClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__DecisionTreeClassifier__1211-21.pkl\n",
      "LogisticRegression\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__LogisticRegression__1211-21.pkl\n",
      "BernoulliNB\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/single__BernoulliNB__1211-21.pkl\n",
      "seeing_someone\n",
      "GradientBoostingClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__GradientBoostingClassifier__1211-21.pkl\n",
      "AdaBoostClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__AdaBoostClassifier__1211-21.pkl\n",
      "RandomForestClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__RandomForestClassifier__1211-21.pkl\n",
      "SVC\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__SVC__1211-21.pkl\n",
      "DecisionTreeClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__DecisionTreeClassifier__1211-21.pkl\n",
      "LogisticRegression\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__LogisticRegression__1211-21.pkl\n",
      "BernoulliNB\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/seeing_someone__BernoulliNB__1211-21.pkl\n",
      "available\n",
      "GradientBoostingClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__GradientBoostingClassifier__1211-21.pkl\n",
      "AdaBoostClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__AdaBoostClassifier__1211-21.pkl\n",
      "RandomForestClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__RandomForestClassifier__1211-21.pkl\n",
      "SVC\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__SVC__1211-21.pkl\n",
      "DecisionTreeClassifier\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__DecisionTreeClassifier__1211-21.pkl\n",
      "LogisticRegression\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__LogisticRegression__1211-21.pkl\n",
      "BernoulliNB\n",
      "El modelo no se entrenó porque ya existía el archivo ./models/available__BernoulliNB__1211-21.pkl\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    print(target)\n",
    "    for modelo in modelos.values():\n",
    "        model_f = {'prep':prep, 'classifier':modelo}\n",
    "        helpers.report_performance(\n",
    "            helpers.pipeline_maker(**model_f),\n",
    "            str(modelo.__class__).replace(\"'>\", '').split('.')[-1],\n",
    "            target,\n",
    "            data[target]['X_train'],\n",
    "            data[target]['X_valid'],\n",
    "            data[target]['y_train'],\n",
    "            data[target]['y_valid']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos de test_cupid con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_25248\\4109272332.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_test = pd.read_sql('SELECT * FROM test_cupid', conn, index_col='indice')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>virgo</th>\n",
       "      <th>taurus</th>\n",
       "      <th>scorpio</th>\n",
       "      <th>pisces</th>\n",
       "      <th>libra</th>\n",
       "      <th>leo</th>\n",
       "      <th>gemini</th>\n",
       "      <th>aries</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_straight</th>\n",
       "      <th>sex_m</th>\n",
       "      <th>smokes_sometimes</th>\n",
       "      <th>smokes_trying_to_quit</th>\n",
       "      <th>smokes_when_drinking</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>body_type_overweight</th>\n",
       "      <th>body_type_regular</th>\n",
       "      <th>education_high_school</th>\n",
       "      <th>education_undergrad_university</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19939</th>\n",
       "      <td>48.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19940</th>\n",
       "      <td>52.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19941</th>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19942</th>\n",
       "      <td>24.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19943</th>\n",
       "      <td>39.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19943 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  height  virgo  taurus  scorpio  pisces  libra  leo  gemini  \\\n",
       "indice                                                                     \n",
       "1       22.0    75.0    0.0     0.0      0.0     0.0    0.0  0.0     1.0   \n",
       "2       32.0    65.0    1.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "3       24.0    67.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "4       29.0    62.0    0.0     1.0      0.0     0.0    0.0  0.0     0.0   \n",
       "5       39.0    65.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "...      ...     ...    ...     ...      ...     ...    ...  ...     ...   \n",
       "19939   48.0    73.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19940   52.0    70.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19941   59.0    62.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19942   24.0    72.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19943   39.0    68.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "\n",
       "        aries  ...  orientation_straight  sex_m  smokes_sometimes  \\\n",
       "indice         ...                                                  \n",
       "1         0.0  ...                   1.0    1.0               1.0   \n",
       "2         0.0  ...                   1.0    0.0               0.0   \n",
       "3         0.0  ...                   1.0    0.0               0.0   \n",
       "4         0.0  ...                   1.0    0.0               0.0   \n",
       "5         0.0  ...                   1.0    0.0               0.0   \n",
       "...       ...  ...                   ...    ...               ...   \n",
       "19939     0.0  ...                   1.0    1.0               0.0   \n",
       "19940     0.0  ...                   1.0    1.0               0.0   \n",
       "19941     0.0  ...                   1.0    0.0               0.0   \n",
       "19942     0.0  ...                   1.0    1.0               0.0   \n",
       "19943     0.0  ...                   0.0    1.0               1.0   \n",
       "\n",
       "        smokes_trying_to_quit  smokes_when_drinking  smokes_yes  \\\n",
       "indice                                                            \n",
       "1                         0.0                   0.0         0.0   \n",
       "2                         0.0                   0.0         0.0   \n",
       "3                         0.0                   1.0         0.0   \n",
       "4                         0.0                   0.0         0.0   \n",
       "5                         0.0                   0.0         0.0   \n",
       "...                       ...                   ...         ...   \n",
       "19939                     0.0                   0.0         0.0   \n",
       "19940                     0.0                   0.0         0.0   \n",
       "19941                     0.0                   0.0         0.0   \n",
       "19942                     0.0                   0.0         0.0   \n",
       "19943                     0.0                   0.0         0.0   \n",
       "\n",
       "        body_type_overweight  body_type_regular  education_high_school  \\\n",
       "indice                                                                   \n",
       "1                        0.0                0.0                    0.0   \n",
       "2                        0.0                0.0                    0.0   \n",
       "3                        0.0                0.0                    0.0   \n",
       "4                        0.0                1.0                    0.0   \n",
       "5                        0.0                0.0                    0.0   \n",
       "...                      ...                ...                    ...   \n",
       "19939                    0.0                1.0                    0.0   \n",
       "19940                    0.0                0.0                    0.0   \n",
       "19941                    0.0                0.0                    0.0   \n",
       "19942                    0.0                0.0                    0.0   \n",
       "19943                    0.0                1.0                    0.0   \n",
       "\n",
       "        education_undergrad_university  \n",
       "indice                                  \n",
       "1                                  1.0  \n",
       "2                                  1.0  \n",
       "3                                  1.0  \n",
       "4                                  1.0  \n",
       "5                                  1.0  \n",
       "...                                ...  \n",
       "19939                              0.0  \n",
       "19940                              1.0  \n",
       "19941                              1.0  \n",
       "19942                              1.0  \n",
       "19943                              0.0  \n",
       "\n",
       "[19943 rows x 98 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_sql('SELECT * FROM test_cupid', conn, index_col='indice')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2.extras as extras\n",
    "def execute_values(conn, df, table):\n",
    "\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "    cols = ','.join(list(df.columns))\n",
    "    # SQL query to execute\n",
    "    query = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "    print(f'\\t\\t{query}')\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"\\t\\tthe dataframe is inserted\")\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO single_svc_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO single_svc_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO single_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO single_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "seeing_someone\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "available\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO available_svc_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO available_svc_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO available_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1211-21.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1211-21.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1211-21.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1211-21.pkl\n",
      "\t\tINSERT INTO available_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "the dataframe is inserted\n"
     ]
    }
   ],
   "source": [
    "queries = {\n",
    "    'Query 1' : ['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'],\n",
    "    'Query 2' : ['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'],\n",
    "    'Query 3' : ['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'],\n",
    "    'Query 4' : ['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism']\n",
    "}\n",
    "cursor_test = conn.cursor()\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    X_test = df_test.drop(columns=[target])\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    import glob\n",
    "    import datetime\n",
    "    # 1211-19 = 12 de Noviembre a las 19 horas.\n",
    "    fecha = datetime.datetime.now().strftime('%d%m-%H')\n",
    "\n",
    "    archivos_de_modelos = glob.glob(f'./models/{target}*_{fecha}.pkl')\n",
    "\n",
    "    queries_result = {}\n",
    "    for key, query in queries.items():\n",
    "        print(f'\\t{key}({query})')\n",
    "        for archivo_modelo in archivos_de_modelos:\n",
    "            print(f'\\t\\tModelo: {archivo_modelo}')\n",
    "            queries_result_for_this_model, target_name, model_name = helpers.create_crosstab(archivo_modelo, X_test, y_test, query)\n",
    "            nombre_tabla = f\"{target}_{model_name.lower()}_{key.lower().replace(' ','')}\"\n",
    "            cols = ' numeric ,'.join(list(queries_result_for_this_model.index.names))+' numeric'\n",
    "            cursor_test.execute(f\"DROP TABLE IF EXISTS {nombre_tabla};\")\n",
    "            cursor_test.execute(f\"CREATE TABLE {nombre_tabla} ({cols}, {target}_yhat numeric);\")\n",
    "            conn.commit()\n",
    "            execute_values(conn, queries_result_for_this_model.reset_index(), nombre_tabla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f3c7bcb2ddd9bbecb49b07cce2dbf7ad24b91a73731a897c825faf176394b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
