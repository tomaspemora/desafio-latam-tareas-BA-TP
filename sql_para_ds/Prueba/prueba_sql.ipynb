{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***SQL Para Data Science - Prueba***.\n",
    "### Nombre(s): Thomas Peet, Braulio Águila, Camilo Ramírez\n",
    "### Generación: G47\n",
    "### Profesores: Alfonso Tobar - Sebastián Ulloa\n",
    "### Fecha: 14-11-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Instrucciones: Para poder correr el notebook es necesario renombrar el archivo .env_example a .env y modificar su contenido agregando los datos de conexión del usuario en postgres`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import helpers\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import  MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_DBHOST')\n",
    "dbname = os.getenv('POSTGRES_DBNAME')\n",
    "drop_all = os.getenv('DROP_DATABASES')=='True'\n",
    "\n",
    "if drop_all:\n",
    "    conn = psycopg2.connect(user=user, host=host, port=5432, password=password, database=dbname)\n",
    "    conn.set_session(autocommit=True)\n",
    "\n",
    "    # Obtención de Cursor\n",
    "    cursor = conn.cursor();\n",
    "    \n",
    "    # Eliminación de base de datos en caso de existir\n",
    "    cursor.execute(\"DROP DATABASE IF EXISTS prueba;\")\n",
    "\n",
    "    # Creación de sentencia para la base de datos\n",
    "    sqlCreateDatabase = \"create database \"+ dbname +\";\"\n",
    "\n",
    "    # Creacion de la base de datos en PostgreSQL\n",
    "    cursor.execute(sqlCreateDatabase);\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención y tratamiento de las columnas del CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_cupid.csv')\n",
    "df.rename(columns = {'hispanic / latin':'hispanic_latin'}, inplace = True)\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "cols = df.columns.to_list()\n",
    "for index, col in enumerate(cols):\n",
    "    cols[index] +=' numeric'\n",
    "    \n",
    "cols = ', '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=dbname, user=user, host=host, port=5432, password=password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Eliminando las tablas train_cupid y test_cupid si existen.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS train_cupid;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS test_cupid;\")\n",
    "\n",
    "# Obteniendo un Cursor para las tablas\n",
    "name_table_train = f\"train_cupid ({cols})\"\n",
    "name_table_test = f\"test_cupid ({cols})\"\n",
    "\n",
    "# Creación de las sentecias para las tablas\n",
    "sqlTable_train = \"create table \"+name_table_train+';'\n",
    "sqlTable_test = \"create table \"+name_table_test+';'\n",
    "\n",
    "# Creando las tablas en PostgreSQL\n",
    "cursor.execute(sqlTable_train)\n",
    "cursor.execute(sqlTable_test)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los datos del CSV a las Tablas creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_cupid.csv', 'r') as f:    \n",
    "    next(f) # Saltar la fila de los encabezados del CSV.\n",
    "    cursor.copy_from(f, 'train_cupid', sep=',')\n",
    "\n",
    "with open('test_cupid.csv', 'r') as f:    \n",
    "    next(f) # Saltar la fila de los encabezados del CSV.\n",
    "    cursor.copy_from(f, 'test_cupid', sep=',')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadiendo columna indice a las tablas train_cupid y test_cupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE train_cupid ADD indice SERIAL PRIMARY KEY;')\n",
    "cursor.execute('ALTER TABLE test_cupid ADD indice SERIAL PRIMARY KEY;')\n",
    "conn.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos de train_cupid con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_5332\\4210961192.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql('SELECT * FROM train_cupid', conn, index_col='indice')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20081 entries, 1 to 20081\n",
      "Data columns (total 98 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   age                             20081 non-null  float64\n",
      " 1   height                          20081 non-null  float64\n",
      " 2   virgo                           20081 non-null  float64\n",
      " 3   taurus                          20081 non-null  float64\n",
      " 4   scorpio                         20081 non-null  float64\n",
      " 5   pisces                          20081 non-null  float64\n",
      " 6   libra                           20081 non-null  float64\n",
      " 7   leo                             20081 non-null  float64\n",
      " 8   gemini                          20081 non-null  float64\n",
      " 9   aries                           20081 non-null  float64\n",
      " 10  aquarius                        20081 non-null  float64\n",
      " 11  cancer                          20081 non-null  float64\n",
      " 12  sagittarius                     20081 non-null  float64\n",
      " 13  asian                           20081 non-null  float64\n",
      " 14  hispanic_latin                  20081 non-null  float64\n",
      " 15  black                           20081 non-null  float64\n",
      " 16  indian                          20081 non-null  float64\n",
      " 17  pacific_islander                20081 non-null  float64\n",
      " 18  native_american                 20081 non-null  float64\n",
      " 19  middle_eastern                  20081 non-null  float64\n",
      " 20  colorado                        20081 non-null  float64\n",
      " 21  new_york                        20081 non-null  float64\n",
      " 22  oregon                          20081 non-null  float64\n",
      " 23  arizona                         20081 non-null  float64\n",
      " 24  hawaii                          20081 non-null  float64\n",
      " 25  montana                         20081 non-null  float64\n",
      " 26  wisconsin                       20081 non-null  float64\n",
      " 27  virginia                        20081 non-null  float64\n",
      " 28  spain                           20081 non-null  float64\n",
      " 29  nevada                          20081 non-null  float64\n",
      " 30  illinois                        20081 non-null  float64\n",
      " 31  vietnam                         20081 non-null  float64\n",
      " 32  ireland                         20081 non-null  float64\n",
      " 33  louisiana                       20081 non-null  float64\n",
      " 34  michigan                        20081 non-null  float64\n",
      " 35  texas                           20081 non-null  float64\n",
      " 36  united_kingdom                  20081 non-null  float64\n",
      " 37  massachusetts                   20081 non-null  float64\n",
      " 38  north_carolina                  20081 non-null  float64\n",
      " 39  idaho                           20081 non-null  float64\n",
      " 40  mississippi                     20081 non-null  float64\n",
      " 41  new_jersey                      20081 non-null  float64\n",
      " 42  florida                         20081 non-null  float64\n",
      " 43  minnesota                       20081 non-null  float64\n",
      " 44  georgia                         20081 non-null  float64\n",
      " 45  utah                            20081 non-null  float64\n",
      " 46  washington                      20081 non-null  float64\n",
      " 47  west_virginia                   20081 non-null  float64\n",
      " 48  connecticut                     20081 non-null  float64\n",
      " 49  tennessee                       20081 non-null  float64\n",
      " 50  rhode_island                    20081 non-null  float64\n",
      " 51  district_of_columbia            20081 non-null  float64\n",
      " 52  canada                          20081 non-null  float64\n",
      " 53  missouri                        20081 non-null  float64\n",
      " 54  germany                         20081 non-null  float64\n",
      " 55  pennsylvania                    20081 non-null  float64\n",
      " 56  netherlands                     20081 non-null  float64\n",
      " 57  switzerland                     20081 non-null  float64\n",
      " 58  mexico                          20081 non-null  float64\n",
      " 59  ohio                            20081 non-null  float64\n",
      " 60  agnosticism                     20081 non-null  float64\n",
      " 61  atheism                         20081 non-null  float64\n",
      " 62  catholicism                     20081 non-null  float64\n",
      " 63  buddhism                        20081 non-null  float64\n",
      " 64  judaism                         20081 non-null  float64\n",
      " 65  hinduism                        20081 non-null  float64\n",
      " 66  islam                           20081 non-null  float64\n",
      " 67  pro_dogs                        20081 non-null  float64\n",
      " 68  pro_cats                        20081 non-null  float64\n",
      " 69  spanish                         20081 non-null  float64\n",
      " 70  chinese                         20081 non-null  float64\n",
      " 71  french                          20081 non-null  float64\n",
      " 72  german                          20081 non-null  float64\n",
      " 73  single                          20081 non-null  float64\n",
      " 74  seeing_someone                  20081 non-null  float64\n",
      " 75  available                       20081 non-null  float64\n",
      " 76  employed                        20081 non-null  float64\n",
      " 77  income_between_25_50            20081 non-null  float64\n",
      " 78  income_between_50_75            20081 non-null  float64\n",
      " 79  income_over_75                  20081 non-null  float64\n",
      " 80  drugs_often                     20081 non-null  float64\n",
      " 81  drugs_sometimes                 20081 non-null  float64\n",
      " 82  drinks_not_at_all               20081 non-null  float64\n",
      " 83  drinks_often                    20081 non-null  float64\n",
      " 84  drinks_rarely                   20081 non-null  float64\n",
      " 85  drinks_socially                 20081 non-null  float64\n",
      " 86  drinks_very_often               20081 non-null  float64\n",
      " 87  orientation_gay                 20081 non-null  float64\n",
      " 88  orientation_straight            20081 non-null  float64\n",
      " 89  sex_m                           20081 non-null  float64\n",
      " 90  smokes_sometimes                20081 non-null  float64\n",
      " 91  smokes_trying_to_quit           20081 non-null  float64\n",
      " 92  smokes_when_drinking            20081 non-null  float64\n",
      " 93  smokes_yes                      20081 non-null  float64\n",
      " 94  body_type_overweight            20081 non-null  float64\n",
      " 95  body_type_regular               20081 non-null  float64\n",
      " 96  education_high_school           20081 non-null  float64\n",
      " 97  education_undergrad_university  20081 non-null  float64\n",
      "dtypes: float64(98)\n",
      "memory usage: 15.2 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql('SELECT * FROM train_cupid', conn, index_col='indice')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentación de la data para los 3 vectores objetivos: [**single**, **seeing_someone**, **available**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['single', 'seeing_someone', 'available']\n",
    "data = {}\n",
    "for target in targets:\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "    data[target] = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'X_train': X_train,\n",
    "        'X_valid': X_valid,\n",
    "        'y_train': y_train,\n",
    "        'y_valid': y_valid,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de los clasificadores a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    'm1' : GradientBoostingClassifier(random_state=42),\n",
    "    'm2' : AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1, n_estimators=5), random_state=42, n_estimators=100, learning_rate=1),\n",
    "    'm3' : RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
    "    'm4' : SVC(random_state=42, probability=True),\n",
    "    'm5' : DecisionTreeClassifier(random_state=42),\n",
    "    'm6' : LogisticRegression(random_state=42, C=0.01),\n",
    "    'm7' : BernoulliNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Pipeline(steps=[\n",
    "    ('num_imp', MeanMedianImputer(imputation_method='mean')),\n",
    "    ('ord', OrdinalEncoder(encoding_method='ordered', variables='age', ignore_format=True)),\n",
    "    ('sc', SklearnTransformerWrapper(StandardScaler(), variables=['age', 'height']))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit y serialización de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.76      0.83      4017\n",
      "weighted avg       0.97      0.96      0.96      4017\n",
      "\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.52      0.68       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.76      0.83      4017\n",
      "weighted avg       0.97      0.96      0.96      4017\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.51      0.68       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.76      0.83      4017\n",
      "weighted avg       0.97      0.96      0.96      4017\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.51      0.68       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.76      0.83      4017\n",
      "weighted avg       0.97      0.96      0.96      4017\n",
      "\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.56      0.53       299\n",
      "         1.0       0.96      0.95      0.96      3718\n",
      "\n",
      "    accuracy                           0.93      4017\n",
      "   macro avg       0.73      0.76      0.74      4017\n",
      "weighted avg       0.93      0.93      0.93      4017\n",
      "\n",
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.51      0.67       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.75      0.83      4017\n",
      "weighted avg       0.96      0.96      0.96      4017\n",
      "\n",
      "BernoulliNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.51      0.68       299\n",
      "         1.0       0.96      1.00      0.98      3718\n",
      "\n",
      "    accuracy                           0.96      4017\n",
      "   macro avg       0.98      0.76      0.83      4017\n",
      "weighted avg       0.97      0.96      0.96      4017\n",
      "\n",
      "seeing_someone\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      0.92      0.96       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      0.96      0.98      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      0.92      0.96       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      0.96      0.98      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "BernoulliNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "available\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      0.92      0.96       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      0.96      0.98      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      0.92      0.96       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      0.96      0.98      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n",
      "BernoulliNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3864\n",
      "         1.0       1.00      1.00      1.00       153\n",
      "\n",
      "    accuracy                           1.00      4017\n",
      "   macro avg       1.00      1.00      1.00      4017\n",
      "weighted avg       1.00      1.00      1.00      4017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    print(target)\n",
    "    for modelo in modelos.values():\n",
    "        model_f = {'prep':prep, 'classifier':modelo}\n",
    "        helpers.report_performance(\n",
    "            # Esta función genera un pipeline con el subpipeline de preprocess y el modelo a entrenar.\n",
    "            helpers.pipeline_maker(**model_f),  \n",
    "            str(modelo.__class__).replace(\"'>\", '').split('.')[-1],\n",
    "            target,\n",
    "            data[target]['X_train'],\n",
    "            data[target]['X_valid'],\n",
    "            data[target]['y_train'],\n",
    "            data[target]['y_valid']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos de test_cupid con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_5332\\4109272332.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_test = pd.read_sql('SELECT * FROM test_cupid', conn, index_col='indice')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>virgo</th>\n",
       "      <th>taurus</th>\n",
       "      <th>scorpio</th>\n",
       "      <th>pisces</th>\n",
       "      <th>libra</th>\n",
       "      <th>leo</th>\n",
       "      <th>gemini</th>\n",
       "      <th>aries</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_straight</th>\n",
       "      <th>sex_m</th>\n",
       "      <th>smokes_sometimes</th>\n",
       "      <th>smokes_trying_to_quit</th>\n",
       "      <th>smokes_when_drinking</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>body_type_overweight</th>\n",
       "      <th>body_type_regular</th>\n",
       "      <th>education_high_school</th>\n",
       "      <th>education_undergrad_university</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19939</th>\n",
       "      <td>48.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19940</th>\n",
       "      <td>52.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19941</th>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19942</th>\n",
       "      <td>24.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19943</th>\n",
       "      <td>39.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19943 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  height  virgo  taurus  scorpio  pisces  libra  leo  gemini  \\\n",
       "indice                                                                     \n",
       "1       22.0    75.0    0.0     0.0      0.0     0.0    0.0  0.0     1.0   \n",
       "2       32.0    65.0    1.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "3       24.0    67.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "4       29.0    62.0    0.0     1.0      0.0     0.0    0.0  0.0     0.0   \n",
       "5       39.0    65.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "...      ...     ...    ...     ...      ...     ...    ...  ...     ...   \n",
       "19939   48.0    73.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19940   52.0    70.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19941   59.0    62.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19942   24.0    72.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "19943   39.0    68.0    0.0     0.0      0.0     0.0    0.0  0.0     0.0   \n",
       "\n",
       "        aries  ...  orientation_straight  sex_m  smokes_sometimes  \\\n",
       "indice         ...                                                  \n",
       "1         0.0  ...                   1.0    1.0               1.0   \n",
       "2         0.0  ...                   1.0    0.0               0.0   \n",
       "3         0.0  ...                   1.0    0.0               0.0   \n",
       "4         0.0  ...                   1.0    0.0               0.0   \n",
       "5         0.0  ...                   1.0    0.0               0.0   \n",
       "...       ...  ...                   ...    ...               ...   \n",
       "19939     0.0  ...                   1.0    1.0               0.0   \n",
       "19940     0.0  ...                   1.0    1.0               0.0   \n",
       "19941     0.0  ...                   1.0    0.0               0.0   \n",
       "19942     0.0  ...                   1.0    1.0               0.0   \n",
       "19943     0.0  ...                   0.0    1.0               1.0   \n",
       "\n",
       "        smokes_trying_to_quit  smokes_when_drinking  smokes_yes  \\\n",
       "indice                                                            \n",
       "1                         0.0                   0.0         0.0   \n",
       "2                         0.0                   0.0         0.0   \n",
       "3                         0.0                   1.0         0.0   \n",
       "4                         0.0                   0.0         0.0   \n",
       "5                         0.0                   0.0         0.0   \n",
       "...                       ...                   ...         ...   \n",
       "19939                     0.0                   0.0         0.0   \n",
       "19940                     0.0                   0.0         0.0   \n",
       "19941                     0.0                   0.0         0.0   \n",
       "19942                     0.0                   0.0         0.0   \n",
       "19943                     0.0                   0.0         0.0   \n",
       "\n",
       "        body_type_overweight  body_type_regular  education_high_school  \\\n",
       "indice                                                                   \n",
       "1                        0.0                0.0                    0.0   \n",
       "2                        0.0                0.0                    0.0   \n",
       "3                        0.0                0.0                    0.0   \n",
       "4                        0.0                1.0                    0.0   \n",
       "5                        0.0                0.0                    0.0   \n",
       "...                      ...                ...                    ...   \n",
       "19939                    0.0                1.0                    0.0   \n",
       "19940                    0.0                0.0                    0.0   \n",
       "19941                    0.0                0.0                    0.0   \n",
       "19942                    0.0                0.0                    0.0   \n",
       "19943                    0.0                1.0                    0.0   \n",
       "\n",
       "        education_undergrad_university  \n",
       "indice                                  \n",
       "1                                  1.0  \n",
       "2                                  1.0  \n",
       "3                                  1.0  \n",
       "4                                  1.0  \n",
       "5                                  1.0  \n",
       "...                                ...  \n",
       "19939                              0.0  \n",
       "19940                              1.0  \n",
       "19941                              1.0  \n",
       "19942                              1.0  \n",
       "19943                              0.0  \n",
       "\n",
       "[19943 rows x 98 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_sql('SELECT * FROM test_cupid', conn, index_col='indice')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para insertar datos en una tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_values(conn, df, table):\n",
    "\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    cols = ','.join(list(df.columns))\n",
    "    \n",
    "    # SQL query que se ejecutará\n",
    "    query = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "    print(f'\\t\\t{query}')\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"\\t\\tthe dataframe is inserted\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries que se deben evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {\n",
    "    'Query 1' : ['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'],\n",
    "    'Query 2' : ['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'],\n",
    "    'Query 3' : ['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'],\n",
    "    'Query 4' : ['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos y guardado de las tablas tabulación cruzada de las predicciones en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO single_svc_query1(atheism,asian,employed,pro_dogs,chinese,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO single_svc_query2(income_over_75,french,german,orientation_straight,new_york,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO single_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\single__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO single_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO single_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO single_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\single__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO single_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,single_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "seeing_someone\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query1(atheism,asian,employed,pro_dogs,chinese,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query2(income_over_75,french,german,orientation_straight,new_york,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\seeing_someone__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\seeing_someone__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO seeing_someone_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,seeing_someone_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "available\n",
      "\tQuery 1(['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO available_svc_query1(atheism,asian,employed,pro_dogs,chinese,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 2(['income_over_75', 'french', 'german', 'orientation_straight', 'new_york'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO available_svc_query2(income_over_75,french,german,orientation_straight,new_york,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 3(['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO available_svc_query3(education_undergrad_university,body_type_regular,pro_dogs,employed,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\tQuery 4(['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'])\n",
      "\t\tModelo: ./models\\available__AdaBoostClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_adaboostclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__BernoulliNB__1311-20.pkl\n",
      "\t\tINSERT INTO available_bernoullinb_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__DecisionTreeClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_decisiontreeclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__GradientBoostingClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_gradientboostingclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__LogisticRegression__1311-20.pkl\n",
      "\t\tINSERT INTO available_logisticregression_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__RandomForestClassifier__1311-20.pkl\n",
      "\t\tINSERT INTO available_randomforestclassifier_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n",
      "\t\tModelo: ./models\\available__SVC__1311-20.pkl\n",
      "\t\tINSERT INTO available_svc_query4(taurus,indian,washington,income_between_50_75,hinduism,available_yhat) VALUES %s\n",
      "\t\tthe dataframe is inserted\n"
     ]
    }
   ],
   "source": [
    "cursor_test = conn.cursor()\n",
    "for target in targets:\n",
    "    print(target)\n",
    "\n",
    "    # Segmentación de la data para el test\n",
    "    X_test = df_test.drop(columns=[target])\n",
    "    y_test = df_test[target]\n",
    "    \n",
    "    # Fecha de los modelos que se utilzaran. Formato: 1211-19 = 12 de Noviembre a las 19 horas. Se utilizará por defecto la fecha actual\n",
    "    fecha = datetime.datetime.now().strftime('%d%m-%H')\n",
    "    \n",
    "    # Listado de los modelos serializados\n",
    "    archivos_de_modelos = glob.glob(f'./models/{target}*_{fecha}.pkl')\n",
    "    \n",
    "    # Se evaluaran las 4 queries en los 7 modelos entrenados para predecir las 3 variables objetivos. Es decir se evaluaran 84 modelos.\n",
    "    for key, query in queries.items():\n",
    "        print(f'\\t{key}({query})')\n",
    "        for archivo_modelo in archivos_de_modelos:\n",
    "            print(f'\\t\\tModelo: {archivo_modelo}')\n",
    "\n",
    "            # Se hace una predicción sobre la data de test y se crea una tabla de tabulación cruzada sobre la data usando las variables de la query\n",
    "            queries_result_for_this_model, target_name, model_name = helpers.create_crosstab(archivo_modelo, X_test, y_test, query)\n",
    "            \n",
    "            # Se genera una tabla usando los datos del vector objetivo, nombre del modelo y query utilizada\n",
    "            nombre_tabla = f\"{target}_{model_name.lower()}_{key.lower().replace(' ','')}\"\n",
    "\n",
    "            # Las columnas que llevará esta tabla son las de la query más una variable y_hat que tendrá el promedio de la predicción para cada combinación\n",
    "            cols = ' numeric ,'.join(list(queries_result_for_this_model.index.names))+' numeric'\n",
    "            \n",
    "            # Se elimina si existe y se crea la tabla\n",
    "            cursor_test.execute(f\"DROP TABLE IF EXISTS {nombre_tabla};\")\n",
    "            cursor_test.execute(f\"CREATE TABLE {nombre_tabla} ({cols}, {target}_yhat numeric);\")\n",
    "            conn.commit()\n",
    "            \n",
    "            # Se insertan los valores de la tabulación cruzada en la tabla\n",
    "            execute_values(conn, queries_result_for_this_model.reset_index(), nombre_tabla)\n",
    "\n",
    "cursor_test.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f3c7bcb2ddd9bbecb49b07cce2dbf7ad24b91a73731a897c825faf176394b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
